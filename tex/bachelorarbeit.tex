% Vorlage für eine Bachelorarbeit
% Siehe auch LaTeX-Kurs von Mathematik-Online
% www.mathematik-online.org/kurse
% Anpassungen für die Fakultät für Mathematik
% am KIT durch Klaus Spitzmüller und Roland Schnaubelt Dezember 2011

\documentclass[12pt,a4paper]{scrartcl}
% scrartcl ist eine abgeleitete Artikel-Klasse im Koma-Skript
% zur Kontrolle des Umbruchs Klassenoption draft verwenden


% die folgenden Packete erlauben den Gebrauch von Umlauten und ß
% in der Latex Datei
\usepackage[utf8]{inputenc}
% \usepackage[latin1]{inputenc} %  Alternativ unter Windows
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage[]{units}
\usepackage{scrpage2}
\usepackage[pdftex]{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage[]{algorithm2e}


% Abstand obere Blattkante zur Kopfzeile ist 2.54cm - 15mm
\setlength{\topmargin}{-15mm}

\newcommand{\changefont}[3]{
\fontfamily{#1} \fontseries{#2} \fontshape{#3} \selectfont}
\changefont{cmr}{m}{n}

% Umgebungen für Definitionen, Sätze, usw.
% Es werden Sätze, Definitionen etc innerhalb einer Section mit
% 1.1, 1.2 etc durchnummeriert, ebenso die Gleichungen mit (1.1), (1.2) ..
\newtheorem{Satz}{Satz}[section]
\newtheorem{Definition}[Satz]{Definition} 
\newtheorem{Lemma}[Satz]{Lemma}	
\newtheorem{Beweis}{Beweis}	
                  
\numberwithin{equation}{section} 

% einige Abkuerzungen
\newcommand{\C}{\mathbb{C}} % komplexe
\newcommand{\K}{\mathbb{K}} % komplexe
\newcommand{\R}{\mathbb{R}} % reelle
\newcommand{\Q}{\mathbb{Q}} % rationale
\newcommand{\Z}{\mathbb{Z}} % ganze
\newcommand{\N}{\mathbb{N}} % natuerliche



\begin{document}
  % Keine Seitenzahlen im Vorspann
  \pagestyle{empty}

  % Titelblatt der Arbeit
  \begin{titlepage}

    \includegraphics[scale=0.45]{logo.png} 
    \vspace*{2cm} 

 \begin{center} \large 
    
    Bachelorarbeit
    \vspace*{2cm}

    {\huge Numerische Analyse des TrueSkill Verfahrens}
    \vspace*{2.5cm}

    Johannes Loevenich
    \vspace*{1.5cm}

    Datum der Abgabe
    \vspace*{4.5cm}


    Betreuung: Prof. Dr. Jochen Garcke \\[1cm]
    Fakultät für Mathematik \\
		Rheinische Friedrich-Wilhelms- Universität Bonn 
  \end{center}
\end{titlepage}



  % Inhaltsverzeichnis
  \tableofcontents

\newpage
 


  % Ab sofort Seitenzahlen in der Kopfzeile anzeigen
  \pagestyle{useheadings}

Das Problem n Spieler zu bewerten ist ein wichtiger Forschungsbereich des Maschinellen Lernens. \\
Aus der Kombinatorik lässt sich schnell herleiten, dass es $n!$ verschiedene Möglichkeiten gibt n Spieler in einem Ranking zu bewerten. 
Ziel ist es das eine Ranking zu finden, welches das tatsächliche Können der einzelnen Spieler am besten wiederspiegelt. 
Nimmt man an, dass alle Rankings gleich wahrscheinlich sind, so würde dies bedeuten, dass $log_{2}(n!) \approx nlog_{2}(n)$ Spielausgänge nötig wären, um die 
korrekte Bewertung zu ermitteln. 
Diese Schranke ist jedoch nur dann aussagekräftig, falls die Ausgänge eines jeden Spiels gleichverteilt sind. Bei vielen Spielen wird durch das matchen von nahezu gleich
starken Gegnern versucht diese Chancengleichheit zu erzwingen. In dieser Arbeit wird eine Wahrscheinlichkeitstheoretische Interpretation dieses Problems betrachtet, weshalb es 
im weiteren Verlauf von Bedeutung ist gewisse Unsicherheiten in Rang eines Spielers zu berücksichtigen. Interessanterweise reduziert sich die minimale Anzahl von aussagekräftigen
Spielen auf $nlog_{2}(m)$, wenn wir annehmen, dass es $m \ll n$ Äquivalenzklassen oder Level gibt. \\
Betrachten wir Spiele bei denen $k$ Teams gegeneinander antreten und bewertet werden, dann erfüllt jedes Spiel $log_{2}(k)$ Ausgänge und es werden nur $\frac{nlog_{2}(n)}{log_{2}(k!)}$
aussagekräftige Spielausgänge benötigt. 
Man wird sehen, dass das hier vorgestellte Verfahren für das betrachtete Problem nahezu optimal ist, also bis auf geringe Abweichungen gegen die oben genannten Schranken konvergiert. 

\section{Motivation}

Skill Ratings erfüllen bei Online Spielen auf Konsolen (z.B. XBOX 360) und im Sport drei hauptsächliche Funktionen. Erstens lassen sich damit interessante und ausbalancierte, teambasierte
Spiele zwischen Spielern mit ähnlichem Skill erzeugen. Zweitens ist es möglich Skills und Rankings für Spieler öffentlich zu machen, um damit zwischen den Spielern einen Wettbewerb zu erzeugen. 
Dirttens können Ratings als Qualifikationskriterium für Turniere verwendet werden. 
Mit dem steigenden Interesse an Online Spielen in den Letzten Jahren, ist auch das Interesse an effektiven Rating Systemen für Modelle mit Millionen Spielern pro Tag 
stark angestiegen.\\
1959 entwickelte Arpad Elo ein statistisches Rating System für Schach Spieler, welches von der \textit{World Chess Federation FIDE} 1970 offiziell anerkannt wurde.
Die grundlegende Idee des \textit{Elo Rating Systems} ist es den möglichen Ausgang eines Spieles als Funktion der beiden Spieler Ratings $s_1$ und $s_2$ zu modellieren. 
In jedem Spiel weißt ein Spieler eine gewisse \textit{Leistungen} $p_i \sim \mathcal{N}(p_i;s_i,\beta^2)$ auf. Es wird angenommen, dass diese \textit{Leistungen} normalverteilt um $s_i$ mit
fester Varianz $\beta^2$ ist. Die Wahrscheinlichkeit, dass Spieler 1 gewinnt entspricht der Wahrscheinlichkeit, dass seine \textit{Leistungen} $p_1$ größer ist, als die \textit{Leistungen} $p_2$ 
seines Gegners

\begin{equation}
 P(p_1 > p_2 | s_1. s_2) = \varPhi(\frac{s_1 - s_2}{\sqrt{2}\beta}).
\end{equation}

$\varPhi$ bezeichnet die kumulative Dichte der Gaußverteilung mit Nullerwartung und Einheitsvarianz.
Nachdem das Spiel beendet ist werden die Skill Ratings der Spieler $s_1$ und $s_2$ so aktualisiert, dass der beobachtete Spielausgang wahrscheinlicher wird. 
Sei $y = +1$, falls Spieler 1 gewinnt, $y = -1$, falls Spieler 2 gewinnt und $y = 0$, falls die beiden Spieler unentschieden spielen. 
Das resultierende \textit{Elo Update} für beide Spieler hat dann die Form $s_1 \leftarrow s_1 + y \Delta$ und 

\begin{equation}
 \Delta = \alpha \beta \sqrt{\pi} (\frac{y+1}{2} \varPhi(\frac{s_1 - s_2}{\sqrt{2} \beta})).
\end{equation}

Viele moderne Ranking Systeme basieren in ihrer Idee auf dem hier kurz vorgestellten \textit{Elo Ranking System}.
Mark Glickman zum Beispiel entwarf basierend auf dem \textit{Elo Ranking System} das sogenannte \textit{Glicko Rating System} um dem  Problem entgegen zu wirken, dass das \textit{Elo Ranking System} an die 20 Spiele
benötigt um aussagekräftige Skill Ratings zu erzeugen. \\

Eine wichtiges neues Anwendungsgebiet für Ranking Systeme sind Online Multiplayer Spiele. Aufgabe ist es unter anderem ausgeglichene, faire Spiele zwischen Spielern mit ähnlichem
Skill zu erzeugen. Multiplayer Online Spiele stellen folgende neue Anforderungen an Ranking Systeme:

\begin{enumerate}
 \item Spielausgänge bestehen meist aus dem Ranking der Teams. Eine Herausforderung ist es, die Ranking Skills einzelner Spieler aus diesen Rankings zu erzeugen.
 \item Es gibt Spiele bei denen mehr als zwei Teams oder Spielern gegeneinander antreten. Bei solchen Spielen gibt es nicht nur einen Gewinner und einen Verlierer. Viel mehr besteht der Spielausgang
 hier aus einer Permutation von Teams oder Spielern. 
\end{enumerate}

In dieser Arbeit werden wir ein Verfahren analysieren, dass diese Anforderungen unterstützt.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newpage  % neuer Abschnitt auf neue Seite, kann auch entfallen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Das Trueskill Verfahren}

\subsection{Das Ranking Problem}

Wir betrachten im Folgenden Spiele, bei denen Spieler in zwei oder mehr Teams gegeneinander antreten. Sind nur zwei Spieler vorhanden, so betrachten wir die beiden Spieler als zwei Teams
die gegeneinander antreten. Falls zwei Spieler oder Teams dieselbe Bewertung erhalten, sagen wir das das Spiel unentschieden ausgegangen ist. 
Im einfachen Fall von nur zwei gegeneinander antretenden Teams, sind nur drei Spielausgänge möglich: Sieg, Niederlage oder unentschieden. \\
Nummerieren wir alle teilnehmenden Spieler eines Spiels von 1 bis n, so kann ein Spiel zwischen $k$ Teams vollständig durch die $k$ Indizes $i_{j} \in \{ 1,...,n \} $ der $n_{j}$ Spieler
im $j$-ten Team beschrieben werden. Der von jedem Team erreichte Rang sei dann definiert als $\textbf{r}:= (r_{1},...,r_{k})^T \in \{ 1,...k \}^{k}$. Es wird davon ausgegangen, dass
dass der Gewinner eines Spiels den Rang 1 erhält. \\
Gesucht wird nach dem Skill $s_{i}$ eines jeden Spielers, $\textbf{s} \in \R^{n}$; wobei der Skill des $j$-ten Teams eine Funktion $S(s_{i_j})$ in Abhängigkeit aller Skills
der Spieler des Teams ist. Im trivialen Fall, dass jedes Team nur einen Spieler enthält beschreibt $S(s_{i}) = s_{i}$ also die Identität.
Für Skills wird die folgende Eigenschaft gefordert. 

\begin{Definition}\textbf{(Stochastische Transitivität)}
Falls ein Team $u$ vor einem Team $v$ platziert wurde, so ist es wahrscheinlicher, dass Team $u$ gegen Team $v$ gewinnt als umgekehrt.

\begin{equation}
 S(s_{i_{u}}) \geq S(s_{i_{v}}) \Rightarrow P(\text{ Team u gewinnt }) > P(\text{ Team v gewinnt })
\end{equation}

\end{Definition}

\subsection{Bayssche Methodik}

Offensichtlich kennen wir den Skill eines Spielers nie mit absoluter Sicherheit. Deshalb basieren das \textit{Trueskill}, sowie viele andere bekannte Verfahren auf Baysschen Modellen. Dazu beschreiben wir den Skill eines
Spielers durch eine Wahrscheinlichkeitsverteilung $P(\textbf{s})$. Wir nehmen außerdem an, dass diese Verteilung einer multivariaten Gaußverteilung entspricht, deren Kovarianzmatrix
Diagonalmatrix mit Einträgen $\sigma_i^2$ ($P(\mathbf{s}) = \mathcal{N}(\mathbf{s},\mathbb{\mu},\mathbf{diag(\sigma^2)})$ ). Diese Annahme hat einige Vorteile: 
\begin{enumerate}
 \item Die Verteilung ist unimodal in $\mu \in \R^n$. Ein Spieler hat deshalb genau einen unbekannten Skill. Dieser schwankt nicht unerwartet zwischen verschiedenen Spielen.
 \item Die Verteilung impliziert einfache Skillupdate Funktionen. (Siehe \textbf{TODO: Kapitel einfügen!})
 \item Die Verteilung kann Speichereffizient implementiert werden. Eine Diagonalmatrix kann leicht als Vektor abgespeichert werden. Der mittlere Skill $\mu_i$ und die Skillvarianz $\sigma_i^2$ sind für jeden Spieler 
 Konstanten.
\end{enumerate}

\subsection{Spielausgangsbasierte Updates}
Sei $\mathbf{r}$ der beobachtete Ausgang eines Spieles. Wir nehmen an, dass $\mathbf{r}$ als Vektor vorliegt, deren Einträge die Platzierungen der Spieler wiederspiegelen. So können wir die Skills aller Spieler $P(\textbf{s})$
mithilfe der Baysschen Regel aktualisieren: 

\begin{equation}
 \begin{split}
 P(s|r,\{i_1,...,i_k\}) &= \frac{P(r|s,\{i_1,...,i_k\}P(s|\{i_1,...,i_k\}))}{P(r|\{i_1,...,i_k\})} \\ 
 &= \frac{P(s|S(s,i_1),...,S(s,i_k)\})P(s)}{P(r|\{i_1,...,i_k\})}
 \end{split}
 \end{equation}

Diese neue Verteilung wird \textit{Posterior} Verteilung genannt und wird im nachfolgenden Spiel für $P(s)$ benutzt.
Diese Art der Vorgehensweise ist als \textit{on-line learning} bekannt: Jederzeit existiert nur eine Verteilung $P(s)$, die durch jeden Spielausgang
beeinflusst wird. Wir nehmen außerdem an, dass die \textit{Posterior} Verteilung alle verfügbaren Informationen enthält. In der Praxis ist es schwer diese Größe exakt, kompakt 
und effizient darzustellen. Deshalb wird sie meist z.B. mithilfe von geeigneten Methoden approximiert. 
Wir stellen in dieser Arbeit zwei unterschiedliche Vorgehensweise dar um dieses Problem zu lösen. Ein auf Faktorgraphen und dem Expectation Propagation Algorithmus basierender Ansatz wird in Kapitel
\textbf{TODO: Kapitel einfügen!} vorgestellt. In Kapitel \textbf{TODO: Kapitel einfügen!} wählen wir einen direkten Ansatz und versuchen auftretende Integrale numerisch möglichst effizient zu approximieren. 

\subsection{Zeitabhängige Updates}

Die bis hierhin beschriebene Vorgehensweise macht eine durchaus starke Annahme: \\
Die Skills aller Spieler, $\mu$, sind zeitunabhängig.\\
Dies ist in der Realität meist jedoch nicht der Fall. Denn Spieler können zum Beispiel mit der Zeit mehr über das Spiel lernen oder spielen eine Zeit lang nicht und 
werden wieder schlechter. 
\\
Wir sind deshalb an einem \textit{Posterior} $P(s_i| \vartriangle t)$, welcher impliziert, dass ein Spieler mit Index $i$ eine Zeit lang $\vartriangle t$ nicht gespielt hat.
Dazu benötigen wir ein Maß $P(\vartriangle \mu | \vartriangle t)$ für die Änderung des Skills $\vartriangle \mu$ des Spielers in der Zeit $\vartriangle t$.
Wir folgen dem \textit{Trueskill} Modell nach Herbrich (Literatur) und nehmen an, dass der Skill eines Spielers in dieser Zeit steigt oder fällt und das Ausmaß der Änderung durch eine Funktion $\tau$ in
Abhängigkeit von $\vartriangle t$ beschrieben wird.
Damit erhalten wir $P(\vartriangle \mu | \vartriangle t) = \mathcal{N}(\vartriangle \mu; 0;\tau^2(\vartriangle t))$.
Übertragen wir dieses Modell auf die Skills $s_i$ der einzelnen Spieler so erhalten wir: 

\begin{equation}
\begin{split}
 P(s_i|\vartriangle t ) &= \int P(s_i| \mu_i + \vartriangle \mu)P(\vartriangle \mu | \vartriangle t) d(\vartriangle \mu) \\
 &= \int \mathcal{N}(s_i; \mu_i + \vartriangle \mu, \sigma_i^2)  \mathcal{N}(\vartriangle \mu; 0, \tau^2(\vartriangle t)) d(\vartriangle y) \\
 &= \int \mathcal{N}(s_i;\mu)\mathcal{N}(\mu; \mu_i, \tau^2(\vartriangle t)) d \mu \\
 &= \mathcal{N}(s_i;\mu_i,\sigma_i^2 + \tau^2(\vartriangle t)) \\
 \end{split}
\end{equation}

Wir werden in dieser Arbeit $\tau$ als konstant $\tau_0$ annehmen.


\subsection{Der \textit{Trueskill} Faktorgraph}
 
Das \textit{Trueskill} Verfahren ist ein Bayssches Ranking Verfahren, dass in seinen Grundzügen eine Erweiterung des
Elo Ranking Systems aus dem Schach ist. 
Dieses neue System beachtet Unsicherheiten in der Bewertung eines Spielers, stellt neue Modelle für ausgeglichene
Spiele auf und kann die Skills eines einzelnen Spielers aus dem Rang eines Teams ermitteln. 
Unsicherheiten werden durch eine Approximierung des message passing in faktorgraphen simuliert.  
\\
 Sei ${1,...,n}$ eine Menge von Spielern, die in $k$ Teams gegeneinander antreten. Die Zuweisung der Teams sei durch k disjunkte Teilmengen
 $A_j \subset \{1,...,n\}$ mit $A_i \cap A_j = \emptyset \text{ für } i \neq j$ eindeutig beschrieben. Der Ausgang des Spiels sei definiert durch $\mathbf{r} := (r_1,...,r_k) \in \{1,...,k\}$,
 wobei $r_j$ der Rang eines jeden Teams $j$ sei. Falls $r_j = 1$, so hat Team $j$ das Spiel gewonnen. Falls $r_i = r_j \text{ für } i \neq j$ so sagen wir, dass Team i gegen Team j
 unentschieden gespielt hat. 
 
 Wir wollen die Wahrscheinlichkeit $P(r|s,A)$ eines bestimmten Spielausganges $\mathbf{r}$ mit gegebenen Skills $\mathbf{s}$ der Spieler und einer Teamzuweisung $A := (A_1,...,A_k)$ modellieren. 
 Mithilfe der Bayschen Regel erhalten wir für die Posterior Verteilung
 
 \begin{equation}
  p(\mathbf{s}|\mathbf{r},A) = \frac{P(\mathbf{r}|\mathbf{s},A)p(\mathbf{s})}{P(\mathbf{r}|A)}.
 \end{equation}

 Dabei nehmen wir an, dass $p(\mathbf{s}) := \prod_{i = 1}^n \mathcal{N}(s_i;\mu_i,\sigma_i^2)$. Dies macht Sinn, da der Faktorgraph selbst eine Menge von Multiplikationen darstellt, um
 eine gemeinsame Verteilung zu finden. 
 Wir weisen außerdem jedem Spieler in einem Spiel eine Leistungen $p_i \sim \mathcal{N}(p_i; s_i,\beta^2)$ mit Zentrum $s_i$ und Varianz $\beta^2$ zu. Die Leistungen $t_j$ eines Teams
 $j$ entspricht der Summe $t_j := \sum_{i \in A_j} p_i$ der Spieler des Teams. 
 
 Ordnen wir die Teams nach ihren Rängen im Spielausgang $\mathbf{r}$ und nehmen wir an, dass unentschieden zunächst nicht zugelassen sind, so erhalten wir
 
 \begin{equation}
  P(\mathbf{r}| \{t_1,...,t_k\}) = P(\mathbf{r}| \{t_r(1),...,t_r(k)\}).
 \end{equation}

 Sind unentschieden zugelassen, so erfordert der Spielausgang $r_(j) < r_(j+1)$ $t_r(1) > t_r(j+1) + \epsilon$ und ein unentschieden der Art $r_(j) = r_(j+1)$ umgekehrt 
 $|t_r(1) - t_r(j+1)| \leq \epsilon$, wobei $\epsilon > 0$ ein Maß für die Gewinnspanne ist.
 
 Wir wollen nun nach jedem Spiel die Skills der Spieler in Abhängigkeit des Spielausganges $\mathbf{r}$ auktualisieren und nutzen dafür die Darstellung mittels Faktorgraphen und
 den daran gebundenen Expectation Propagation Algorithmus. 
 Dazu approximieren wir die Posterior Verteilungen gaußverteilt und verwenden sie im darauf folgenden Spiel als Prior Verteilungen. 
 
 Um das Verfahren an einem Beispielgraphen herleiten und erklären zu können, betrachten wir folgendes Spiel. 
 Wir nehmen an, dass $k = 3$ Teams mit $A_1 = \{1\}, A_2 = \{2,3\} \text{ und } A_3 = \{4\}$ gegeneinander antreten. 
 Der Spielausgang sei $\mathbf{r} = (1,2,2)$. Dies bedeutet, dass Team 1 das Spiel gewonnen hat und Team 2 und Team 3 unentschieden gespielt haben. 
 In Figur () geben wir den korrespondierenden \textit{Trueskill} Faktor Graphen an.
 
 Wie bereits in Kapitel () beschrieben, ist die grundlegende Idee eine gemeinsame Wahrscheinlichkeitsverteilung von mehreren Zufallsvariablen 
 in zwei verschiedene Typen von Knoten aufzuteilen. Schwarze Boxen stellen Faktorknoten und Kreise Variablenknoten dar. 
 Die gemeinsame Verteilung ergibt sich dann als Produkt der einzelnen Faktoren: 
 
 \begin{equation}
  p(X) = \frac{1}{Z} \prod_S f_S(X_S)
 \end{equation}
 
 $X_S$ beschreibt die zu einem Faktor $f_S$ zugehörigen Variablen und $Z$ sei Normalisierungskonstante. 
 Das wichtigste Konzept ist dann die Anwendung der Summen-Produkt-Update Regel: 
 
 \textit{Jede Nachricht, die von einem Knoten v über eine Kante e übermittelt wird, entspricht dem Produkt der lokalen Funktion an v (Oder der Einheitsfunktion, falls v Variablenknoten
 ist) mit allen eingegangenen Nachrichten an v außer an Kante e.}
 
 In Anlehnung an Kapitel () ergeben sich damit die drei folgenden Gleichungen
 
 \begin{equation}
  p(v_k) = \prod_{f \in F_{v_k}}m_{f \rightarrow v_k}(v_k)
 \end{equation}
 
 Gleichung () sagt uns, dass der Wert des Marginales an $v_k$ dem Produkt der eingehenden Nachrichten entspricht. 
 
 \begin{equation}
  m_{f \rightarrow v_j}(v_j) = \int ... \int f(v) \prod_{i \neq j} m_{v_i \rightarrow f}(v_i) d \mathbf{v}_{\backslash j}
 \end{equation}

 Gleichung () zeigt, dass der Wert einer Nachricht von einem Faktor $f$ zu einer Variable $v_j$ der Summe über das Produkt aller anderen Nachrichten zwischen diesem Faktor
 und abhängigen Variablen außer $v_j$ entspricht. 
 
 \begin{equation}
  m_{v_k \rightarrow f}(v_k) =  \prod_{f' \in F_{v_k}\backslash \{f\} }m_{f' \rightarrow v_k}(v_k)
 \end{equation}

 Gleichung () beschreibt, dass die Nachricht von einer Variable $v_k$ zu einem Faktor $f$ das Produkt aller anderen Nachrichten ist. 
 
 Für weitere Details möchten wir auf Kapitel () und die dortigen Bespiele verweisen. 
 
 \subsubsection{Prior Faktoren}
 
 Betrachten wir die erste Ebene des \textit{Trueskill} Graphen Prior Faktor ist im \textit{Trueskill} Graphen als schwarze Box dargestellt. 
 
 % Prior Bild
 
 Um die \textit{Trueskill} Update Formeln vereinfacht darstellen zu können, verwenden wir die folgenden Notationen
 
 \begin{equation}
  \begin{split}
    \pi &= \sigma^{-2} = \frac{1}{\sigma^2} \\
    \tau &= \pi \mu = \frac{\mu}{\sigma^2} \\
   \end{split} 
 \end{equation}
 
 Das Ziel des Prior Faktors ist es diese Werte einer gaußverteilten Zufallsvariable mit Erwartung $m$ und Varianz $v\nu^2$ zu aktualisieren. 
 Mit obiger Notation ergibt sich also:
 
 \begin{equation}
  \begin{split}
    \pi_x^{\text{new}} &\leftarrow \pi_x + \frac{1}{\nu^2} \\
    \tau_x^{\text{new}} &\leftarrow \tau_x + \frac{m}{\nu^2} \\
  \end{split}
 \end{equation}

  Wir sehen, dass die beiden Gleichungen den ersten Gleichungen im \textit{Trueskill} Paper () entsprechen. 
  
  \subsubsection{Unsicherheiten}
  
  Wie bereits in der Einleitung erwähnt können Skills nur mit einer gewissen Unsicherheit beschrieben werden. Wir beschreiben hier, wie solche Unsicherheiten im \textit{Trueskill} Verfahren
  berücksichtigt werden. 
  Dazu nehmen wir an, dass wir eine gauqverteilte Zufallsvariable $y$ gegeben haben und eine neue gaußerverteilte Zufallsvariable $\mathcal{N}(x;y,c^2)$ erhalten wollen, die eine gewisse Unsicherheit
  $c^2$ berücksichtigt.  
  Dazu sei $a := (1+c^2(\pi_y - \pi_{f \rightarrow y}))^{-1}$.
  Durch Umformungen erhalten wir
  \begin{equation}
  \begin{split}
   a &= (1+c^2(\pi_y - \pi_{f \rightarrow y}))^{-1} = \frac{1}{1+c^2(\pi_y - \pi_{f \rightarrow y})} \\
   &\approx \frac{\sigma_y^2}{\sigma_y^2 + c^2} \\
  \end{split}
  \end{equation}
  
  Es ist somit leicht zu sehen, dass a stets kleiner als 1 ist. Nun kann die Ungewissheit $c^2$ approximativ den Werten $\pi$ und $\tau$ zugewiesen werden: 
  
  \begin{equation}
   \begin{split}
      \pi_{f \rightarrow x}^{\text{new}} &\leftarrow a(\pi_y - \pi_{f \rightarrow y}) = \frac{1}{\sigma_y^2 + c^2} \\
      \tau_{f \rightarrow x}^{\text{new}} &\leftarrow a(\tau_y - \tau_{f \rightarrow y}) = \frac{\mu_y}{\sigma_y^2 + c^2} \\
   \end{split}
  \end{equation}

  \subsubsection{Teamperformanz}
  Dieser Faktor wird benutzt um die Differezen zwischen Teams innerhalb eines Spieles auszudrücken und summiert mehrere gaußverteilte Variablen miteinander.
  Wir nehmen an, dass wir n Variablen $v_1,...,v_n$ miteinander summieren wollen, wobei jede dieser Variablen mit einem Faktor $a_n$ gewichtet wird und $v_0$ dem Wert der
  Summe entspricht. Es gilt also: 
  
  \begin{equation}
   v_0 = a_1v_1 + ... + a_nv_n 
  \end{equation}
  
  Aus Kapitel () über die Gaußverteilung wissen wir, dass dies einer Zufallsvariable mit Erwartungswert und Varianz der Form
  
  \begin{equation}
   \begin{split}
   \mu &= \sum_{i = 1}^n a_i \mu_i \\
   \sigma^2 &= \sum_{i = 1}^n a_i^2\sigma_i^2 \\ 
   \end{split}
  \end{equation}
  
  entpricht. Da die Update Formeln durch den Gebrauch der oben eingeführten Notation mithilfe von $\pi$ und $\tau$ einfacher und kompakter zu implementieren sind, 
  müssen diese in eine solche Form gebracht werden. 
  
  \begin{equation}
   \pi^{new} = \frac{1}{\sigma^2} = \frac{1}{\sum_{j=1}^{n} a_j^2 \sigma_j^2 = (\sum_{j=1}^n \frac{a_j^2}{\pi_j})^{-1}
  \end{equation}

  Für den precision mean müssen wir den $\pi^{new}$ mit mit dem Erwartungswert multiplizieren: 
  
  \begin{equation}
   \tau^{new} = \pi^{new} \cdot \mu = \pi^{new} \cdot (\sum_{j=1}^n a_j \mu_j).
  \end{equation}

  

   
   \subsection{Multiple Team Games Algorithms}
   
   In diesem Abschnitt beschreiben wir die Funktionsweise von Update Algorithmen für Spiele, in denen mehrere 
   Teams gegeineinander antreten. 
   Wir nehmen an, dass in einem Spiel $k$ Teams gegeinander antreten, wobei in Team $j$ $n_j$ mit Indizes $\mathbf{i}_j$ Spieler zusammen
   spielen. Wir gehen außerdem davon aus, dass wir den Spielausgang in Form eines Ranking Vektors
   $\mathbf{r} := (r_1,...,r_k) \in \{1,...,k\}$ vorliegen. Dies erlaubt es uns die Teams in der Reiheinfolge
   $ r_{(1)} \leq r_{(2)} \leq ... \leq r_{(k)}$ neu zu ordnen. Dabei bezeichnet $( \cdot )$ eine Permutation 
   der Indizes von $1,...,k$ bezeichnet, welche die Ordung der Rankings der Team in der gewünschten Reihenfolge
   erzeugt. Somit ist es möglich den Spielausgang $\mathbf{r}$ mittels Permutation von Teamindizes und einem Vektor 
   $\mathbf{y} \in \{0, +1\}^{k-1}$ zu schreiben. $\mathbf{y}$ beschreibt dabei, wie Team $j$ und Team $j+1$ gegeneinander
   gespielt haben. Falls  Team $j$ gegen Team $j+1$ gewonnen hat gelte $(y_j = +1)$, bei einem Unentschieden gelte 
   $(y_j = 0)$, d.h $y_j = sign(r_{(j+1)} - r_{(j)})$.
   
   \subsection{Das Modell}
   
   Unsere Grundannahme ist es, dass jeder Spielausgang auf den Leistungen der teilnehmenden Spielern aufbaut. Die 
   Leistung $x_i$ eines Spielers sei dabei gaußverteilt, sodass der Skill des Spielers $i$, $s_i$ der Erwartung der Gaußverteilung entspricht. 
   Wir nehmen außerdem an, dass die Varianz der Leistung einzelner Spieler der Konstante $\beta^2$ entspricht. Damit folgt $x_i \sim \mathcal{N}(x_i; s_i, \beta^2)$.
   Die Leistung $t(\mathbf{i})$ eines Teams mit Indexmenge $\mathbf{i}$ der Spieler sei lineare Funktion der Leistungen der individuellen Spieler, $t(\mathbf{i}) = \mathbf{b(i)^T x}$.
   Sei eine Stichprobe der Leistungen $\mathbf{x} \in \R^n$ gegeben, so sei das Ranking so definiert, dass das Team mit der größten Leistung auf Rang 1 steht und das Team mit der
   schlechtesten Leistung an letzter Stelle steht. Sollten zwei Teams, bis auf einen minimalen Unterschied $\epsilon$, die gleiche Leistung erbracht haben so nennen wir dies ein Unentschieden.
   Nachdem wir die Leistungen der Spieler nach ihren Werten neugeordnet haben betrachten wir stets nur die paarweise Differenz, um zu entscheiden, ob das Team mit größerer Leistung
   gewinnt, oder ein Unentschieden vorliegt. \\
   
   Um dieses Modell mathematisch zu formulieren, definieren wir einen $k-1$-dimensionalen Vektor aus Hilfsvariablen $\mathbf{z}$, wobei $z_j := t(\mathbf{i}_j) - t(\mathbf{i}_{j+1}) = \mathbf{a}_j^T \mathbf{x}$
   
   \begin{equation}
    \mathbf{z} := \mathbf{A}^T \mathbf{x} = [a_1 ... a_{k-1}] \mathbf{x}.
   \end{equation}
   
   Da $\mathbf{x} \sim \mathcal{N}(\mathbf{x};\mathbf{s}, \beta^2 \mathbf{I})$ gaußverteilt ist, wissen wir das $\mathbf{z}$ ebenfalls gaußverteilt ist mit
   $\mathbf{z} \sim \mathcal{N}(\mathbf{z}; \mathbf{A}^T \mathbf{s}, \beta^2 \mathbf{A} \mathbf{A}^T) $.
   Damit können wir die Wahrscheinlichkeit eines Rankings $r$ mittels der gemeinsamen Verteilung über $\mathbf{z}$ beschreiben, 
   
   \begin{equation}
    P(\mathbf{y}| \mathbf{s_i}_1,...,\mathbf{s_i}_k) = \prod_{j = 1}^{k-1} (P(z_j > \epsilon))^{y_i}(P(|z_j| \leq \epsilon))^{1 - y_j}.
   \end{equation}
   
   In Kapitel () beschreiben wir Methoden, wie solche Wahrscheinlichkeiten numerisch berechnet werden können. 

   \subsection{Vorhersage Algorithmus}
   
   In dieser Phase sind wir daran interessiert, die Wahrscheinlichkeit eines bestimmten Spielausganges $\mathbf{r}$ vorherzusagen, falls die Skills $\mathbf{\mu}$ und die
   Standardabweichungen $\mathbf{\sigma}$ der Spieler bekannt sind. Dazu müssen wir $P(\mathbf{y})$ aus $P(\mathbf{y}|s_{\mathbf{i}}_1,...,s_{\mathbf{i}}_k)$ wo alle Ungewissheiten
   über die Skills $s_{\mathbf{i}}_1,...,s_{\mathbf{i}}_k$ ausmarginalisiert werden. Der Algorithmus ergibt sich leicht aus den in Kapitel () und () vorgestellten Methoden.
  
   \subsection{Asymptotische Analyse }
   
   In diesem Kapitel untersuchen wir das asymptotische Verhalten des \textit{Trueskill} Verfahrens. Hauptsächlich werden wir die Konvergenz der Skillparameter $\mu$ und $\sigma$ untersuchen.
   Während Ersterer von den reellen und nicht bekannten Wahrscheinlichkeiten der Spielausgänge zwischen allen Spielen abhängt, wird der Parameter $\sigma$ stets gegen einen festen
   Wert konvergieren, da jeder neue Spielausgang zu einer Verringerung der Entropie des Skills führt.  
   
   \subsubsection{$\sigma$ Analyse}
   
   Um den asymptotischen Wert von $\sigma$, den wir mit $\sigma*$ bezeichnen wollen, zu untersuchen machen wir die drei folgenden Annahmen: 
   
   \begin{enumerate}
    \item Im asymtotischen Grenzwert haben alle Spielerskills die gleiche Standardabweichung $\sigma = \sigma* \textbf{1}$.
    \item Im asymptotischen Grenzwert werden nur Spieler gegeneinander antreten, die gleiche Skills haben. 
    \item Für jede Paarung von Teams werden die Spiele nicht unentschieden ausgehen.
   \end{enumerate}
   
   Zur Analyse betrachten wir den Update Algorithmus aus Kapitel (). 
   Da wir den asymptotischen Grenzwert als Grundvorraussetzung annehmen, wissen wir dass eine Aktualisierung der $\sigma*$ Werte diesen Grenzwert unverändert lassen. 
   
   \begin{equation}
   \begin{split}
    \sigma*^2 &= min_t [(\sigma*^2 + \tau_0^2) (1- (\sigma*^2 + \tau_0^2)) W_{t,t}] \\
    &= (\sigma*^2 + \tau_0^2)(1-(\sigma*^2 + \tau_0^2) max_t(W_{t,t})). \\
    \end{split}
   \end{equation}
   
   Wir merken an, dass $W_{t,t}$ von $\beta^2, t_0^2$ und von $\sigma*^2$ abhängt. Wir wissen aus Algorithmus (), dass im asymptotischen Grenzwert für $\mathbf{C}$ und $\mathbf{Z}$
   gilt:
   
   \begin{equation}
    \begin{split}
      \mathbf{C} &= (\beta^2 + \sigma*^2 + \tau_0^2) \mathbf{A^T} \mathbf{A} = (\beta^2 + \sigma*^2 + \tau_0^2) \mathbf{C}_0, \\
      \mathbf{Z} &= (\beta^2 + \sigma*^2 + \tau_0^2) \mathbf{Z}_0 \\
    \end{split}
   \end{equation}
   
   Dabei bezeichnet $\mathbf{Z}_0$ die Kovarianz einer abgeschnittenen Gaußverteilung mit Erwartung $\mathbf{0}$, Kovarianz $\mathbf{C}_0$ und Integrationsgrenzen
   $(\epsilon \mathbf{1}, + \mathbf{\infty})$. Dann gilt: 
   
   \begin{equation}
    \begin{split}
     \mathbf{W} &= \mathbf{A} \mathbf{C}^{-1} ( \mathbf{C} - \mathbf{Z} ) \mathbf{C}^{-1} \mathbf{A}^T \\
     &= (\beta^2 + \sigma*^2 + \tau_0^2) \mathbf{A} \mathbf{C}_0^{-1} ( \mathbf{C}_0 - \mathbf{Z}_0 ) \mathbf{C}_0^{-1} \mathbf{A}^T. \\
    \end{split}
   \end{equation}
   
   Nehmen wir an, dass $w_0 = max_t[\mathbf{A} \mathbf{C}_0^{-1} ( \mathbf{C}_0 - \mathbf{Z}_0 ) \mathbf{C}_0^{-1} \mathbf{A}^T]_{t,t} $, so
   ist der asymptotische Grenzwert vin $\sigma$ implizit gegeben durch: 
   
   \begin{equation}
    \sigma*^2 = \tau_0 \cdot (\sigma*^2 + \tau_0^2) (1 - \frac{ \sigma*^2 + \tau_0^2 }{ \beta^2 + \sigma*^2 + \tau_0^2 } w_0).
   \end{equation}
   
   Dies entspricht einer quadratischen Gleichung in $\sigma*^2$ mit Lösung 
   
   \begin{equation}
    \sigma*^2 = \tau_0^2 \cdot [\frac{1}{2 w_0} + \sqrt{ \frac{1}{2 w_0} (\frac{1}{2 w_0} + \frac{2 \beta^2}{\tau_0^2}) } -1].
   \end{equation}

  \subsection{Matchmaking and Scoreboards}
  
  In diesem Abschnitt beschreiben wir zwei mögliche Erweiterungen des \textit{Trueskill} Verfahrens: Matchmaking and Scoreboards.
  Beide Erweiterungen sind besonders in \textit{Online-Video-Spielen} von großer Bedeutung. 
  
  \subsubsection{Matchmaking}
  
  Die Hauptaufgabe im Matchmaking ist es Spiele zu identifizieren, sodass die Spieler in den Spielen möglichst zufrieden sind. 
  Natürlich ist meist das gewinnende Team zufrieden, aber Spiele zu erzeugen in denen jeder Spieler gewinnt ist unmöglich. 
  Wir definieren deshalb ein zufriedenstellendes Spiel, als ein solches Spiel, in dem jeder Spieler die faire Chance hat das Spiel 
  zu gewinnen. Anders ausgedrückt, ein zufriedenstellendes Spiel, ist ein Spiel in dem die Wahrscheinlichkeit eines Unentschiedens sehr groß ist. \\
  Wir entfernen die Abhängigkeit auf den \textit{Draw Margin} $\epsilon$ durch die Annahme dass im Grenzwert $\epsilon \rightarrow 0$ gilt. 
  Sind die Erwartungen und Kovarianzen der Spielerskills durch $\mu$ und $\varSigma$ gegeben, so gilt: 
  
  \begin{equation}
   \begin{split}
    P(draw| \mu, \varSigma) &= \lim_{\epsilon \rightarrow 0} \int_{- \epsilon \mathbf{1}}^{+ \epsilon \mathbf{1}} \mathcal{N}(\mathbf{z;A^T \mu; A^T (\beta^2 I + \varSigma)A}) d \mathbf{z} \\
    &= \mathcal{N}(\mathbf{0; A^T \mu; A^T(\beta^2 I + \varSigma) A}).
   \end{split}
  \end{equation}
  
  Die Matrix $A$ beschreibt das Spiel, wie in () beschrieben. Um dieses Maß für jedes Spiel vergleichen zu können, vergleichen wir diese Wahrscheinlichkeit mit der Wahrscheinlichkeit
  eines Unentschiedens im gleichen Spiel, wobei alle Teams den gleichen Skill haben, d.h $\mathbf{A^T \mu = 0}$. Damit kann das Maß für die Qualität eines Spieles durch
  
  \begin{equation}
   \begin{split}
    q_{draw}(\mathbf{\mu,\varSigma,\beta,A}) &:= \frac{\mathcal{N}(\mathbf{0;A^T \mu; A^T (\beta^2 I + \varSigma) A})}{\mathcal{N}(\mathbf{0;0; \beta^2 A^T A})} \\
    &= exp(-\frac{1}{2} \mathbf{\mu^T A (\beta^2 A^T A + A^T \varSigma A)}^{-1} \mathbf{A^T \mu}) \cdot \sqrt{ \frac{|\beta^2 \mathbf{A^T A}|}{|\beta^2 \mathbf{A^TA + A^T \varSigma A}|} }. \\
   \end{split}
  \end{equation}
  beschrieben werden.
  Dieses Maß hat die Eigenschaft, dass es stets zwischen $0$ und $1$ liegt. Nimmt es den Wert $1$ an, so bedeutet dies ein perfektes Spiel. In dem Fall, dass alle Spieler
  in $k$ Teams aufgeteilt sind und vorher noch nie ein Spiel gespielt haben, so nimmt das Maß den Wert
  
  \begin{equation}
   exp(- \frac{\mu_0^2}{2(\beta^2 \sigma_0^2)} \mathbf{1_i^T A (A^T A)^{-1} A^T 1_i}) \cdot \frac{\beta^k}{\sqrt{(\beta^2 + \sigma_0^2)^k}}
  \end{equation}

  an. 
  
  \subsubsection{Matchmaking Qualität}
  
  Für die folgende Analyse betrachten wir den Spezialfall, dass zwei Spieler $i$ und $j$ gegeneinander antreten. In diesem Fall kann das Maß der Spielqualität mithilfe der Differenz
  der beiden Erwartungen der Spielerskills und der Summe der Varianzen geschrieben werden. Sei $m_{ij} := \mu_i - \mu_j$ und $\zeta_{ij}^2 := \sigma_i^2 + \sigma_j^2$. Dann gilt:
  
  \begin{equation}
   q_{draw}(m_{ij}, \zeta_{ij}^2, \beta) = exp(- \frac{m_{ij}^2}{2(2 \beta^2 + \zeta_{i,j}^2)}) \cdot \sqrt{ \frac{2 \beta^2}{2 \beta^2 + \zeta_{ij}^2}}.
  \end{equation}

  Um das Maß () zu untersuchen vergleichen wir dieses, mit den Maßen der (erwarteten) Skilldifferenzen, 
  
  \begin{equation}
   \begin{split}
    q_1(m_{ij}, \zeta_{ij}^2, \beta) &= exp(-E[|s_i-s_j|]) \\
    &= exp(-(m_{ij} ( 2 \Phi(\frac{m_{ij}}{\zeta_{ij}})-1) + \zeta_{ij} \mathcal{N}(\frac{m_{ij}}{\zeta_{ij}})), \\
    q_1(m_{ij}, \zeta_{ij}^2, \beta) &= exp(-E[|s_i-s_j|^2]) \\
    &= exp(- (m_{ij}^2 + \zeta_{ij}^2)).
   \end{split}
  \end{equation}
  
  All diese Maße haben die Eigenschaft, dass für Spieler mit gleicher Erwartung, $m_{ij} = 0$, und dass Ungewissheiten in den Skills die Spielqualität negativ beeinflussen. \\
  Um ein Spiel am Anfang zu akzeptieren wählen wir als Qualitätsschwellenwert $q(0,2 \sigma_0^2, \beta)$. \\
  Wenn die Skills der Spieler konvergiert sind aktzeptieren wir dann ein Spiel sobald die Differenz der Erwartungen der Spielerskills $q(m_{ij},0, \beta) = q(0,2 \sigma_0^2, \beta)$ erfüllt. 
  Lösen wir diese Gleichung nach $m_{ij}$, so erhalten wir die Gewinnwahrscheinlichkeit des besseren Spielers.
  Gehen wir von () aus, so erhalten wir z.B, 
  
  \begin{equation}
   m_{ij} = \sqrt{2} \beta \sqrt{\ln(1 + \frac{\sigma_0^2}{\beta^2})} \Leftrightarrow P(\text{better wins}) = \Phi(\sqrt{1+ \frac{\sigma_0^2}{\beta^2}}).
  \end{equation}
  
  Anschaulich ist diese Wahrscheinlichkeit in Abbildung () dargestellt. Wir können sehen, dass das Maß für die Spielqualität deutlich besser ist, als das Maß für die erwarteten Skill 
  Differenzen. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newpage  % neuer Abschnitt auf neue Seite, kann auch entfallen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Faktorgraphen}
Ein Faktorgraph ist ein bipartiter Graph, der beschreibt wie eine globale Funktion in Abhängigkeit von 
verschiedenen Variablen und Faktoren durch ein Produkt von lokalen Funktionen ausgedrückt werden kann.

\subsection{Allgemeines}
In Faktorgraphen wird zwischen zwei Typen von Knoten unterschieden: Die einen Knoten, welche mit Variablen 
identifiziert werden (nicht ausgefüllte Knoten) und die Anderen, welche mit lokalen Funktionen identifiziert werden 
(ausgefüllte Knoten).  Kanten verbinden Variablenknoten $x_{i}$ und Funtionenknoten $f$ genau dann, wenn 
$x_{i}$ Argument von $f$ ist. \\

Sei $X = \{ x_{i} \}_{i \in \N}$ eine Menge von Variablen bzgl. der Indexmenge $N = \{ 1,2,3,...,n \}$. 
Falls $E$ triviale Teilmenge von N ist, so bezeichne mit $X_{E}$ die Teilmenge von $X$, welche durch E induziert wird. 
Für jedes $ i \in N $ nehme die Variable $x_{i}$ Werte aus dem Alphabet $A_{i}$ an. Desweiteren wird angenommen,dass
$A_{i}$ für alle $i \in N$ stets endlich ist. Bezeichne eine bestimmte Belegung der Variablen aus $X$ als Konfiguration 
der Variablen. Diese Konfigurationen können als Kartesisches Produkt $W = \prod_{i \in N}A_{i}$, den sogenannten Konfigurationsraum
verstanden werden. 

Ein Element $w = (w_{1},...,w_{n}) \in W$, mit $w_{i} \in A_{i}$ ist entspricht der Varaiblenbelegung
$x_{1} = w_{1},...,x_{n} = w_{n} $. \\

Im weiteren sind Funktionen mit Urbild $W$ von besonderem Interesse. Sei $g: W \rightarrow R$ eine solche Funktion, auch gloabele
Funktion genannt. Im Moment beziehe sich der Wertebereich auf die Reellen Zahlen. Im Allgemeinen sei jedoch jeder beliebige Semiring erlaubt. \\

\subsection{Beispiel}

Beispielsweise jeder lineare Binärblockcode kann durch eine Menge von Paritätstestgleichungen beschrieben werden, sodass jede
Gleichung eine Bedingung an das Codewort $x = (x_1,...,x_n)$ beschreibt, z.B, $\sum_{i \in E} x_i = 0$. 
Der zu einem Binärcode mit der angegebenen Paritätsmatrix  

\begin{equation}
H =
  \begin{pmatrix}
  1 & 1 & 0 & 1 & 0 & 0 & 0  \\
  0 & 0 & 1 & 1 & 0 & 1 & 0  \\
  0 & 0 & 0 & 1 & 1 & 0 & 1 	
 
 \end{pmatrix}
\end{equation}
korrespondierende Faktorgraph wird in Figur () gezeigt.

\subsection{Indikatorfunktion und Posterior Wahrscheinlichkeit}

Anlehnend an das vorherige Beispiel, sei $(x_1,...,x_n)$ ein gleichverteilt gezogenes Codewort, dass über einen Speicherlosen Kanal 
übermittelt werden soll und $y = (y_1,...,y_n)$ die gesamte Ausgabe des Kanals. Die gemeinsame \textit{a posterior} Wahrscheinlichkeit
vin $\{x_1,...,x_n\}$ kann dann durch eine Funktion $f$ ausgedrückt werden

\begin{equation}
 f(x_1,...,x_n) = \prod_{E \in Q} f_E(x_E) \prod_{i=1}^n f(y_i|x_i).
\end{equation}

Dabei ist für jeden Wert von $x_i$, $f(y_i|x_i)$ die korrespondierende Likelyhood Funktion am entsprechenden Ausgang $y_i$ des Kanals.
Die Faktorgraphdarstellung dieses Szenarios ist in Figur (b) dargestellt.  

\subsection{Bayssche Netwerke}

Bayssche Nezwerke sind gerichtete, azyklische graphische Modelle auf einer Menge von Zufallsvariablen. 
Jeder Knoten $v$ eines Baysschen Netwerkes wird mit einer Zufallsvariable assoziert. Bezeichnet $a(v)$, die Menge
der Vorgänger des Knotens $v$, so hat die Wahrscheitsverteilung des Baysschen Netzwerkes die Form

\begin{equation}
 p(v_{1},v_{2},...,v_{n}) = \prod_{i=1}^n p(v_{i}|a(v_i))
\end{equation}

Falls $a(v_i) = \varnothing $ (d.h. $v_i$ hat keine Vorgänger), so setze $p(v_i| \varnothing) = p(v_i)$.
Figur \textbf{FIGUR EINFÜGEN} zeigt beispielsweise eine Bayssches Netwerk, welches die Faktorisierung 
$$
p(v_1,v_2,v_3,v_4,v_5) = p(v_1|v_2)p(v_2)p(v_3|v_2,v_4)p(v_4)p(v_5|p_4)
$$
beschreibt. 

\subsection{Summen-Produkt-Algorithmus}

Sei $g(X)$ globale Funktion über die Variablen der Menge $X = \{ x_i : i \in N \}$, wobei Variable $x_i$ Werte
der endlichen Menge $A_i$ annimmt.
In diesem Abschnitt wird eine Algorithmus beschrieben, um die marginal Funktionen 

\begin{equation}
 G_i(x_i) = \sum_{x_1 \in A_1,...,x_{i-1} \in A_{i-1},x_{i+1} \in A_{i+1},...,x_n \in A_n} g(x_1,...,x_n)
\end{equation}

für Variablen $x_i, i \in N $ zu berechnen. Im Weiteren sei $\sum_{x_i} f(x_i) = \sum_{x_i \in A_i} f(x_i)$ und genauso
sei für eine Teilmenge $J \subset N$ mit $\sum_{x_i; i \in J}$ f(X) die Summe über alle möglichen Konfigurationen der Variablen $x_i$
über $J$ gemeint. Damit gilt $G_i(x_i) = \sum_{x_j;j\in N \ \{i\}}g(X)$.
Die Definition der marginalen Funktion $G$ kann nun auf eine Teilmenge $J$ von $N$ ausgeweitet werden.

\begin{equation}
 G_i(x_i) = \sum_{x_i; i \in N \ J} g(X).
\end{equation}

Falls $g(x_1,...,x_n)$ eine Wahrscheinlichkeitsverteilung beschreibt, so ist $G_i(x_i)$ die Marginalverteilung und 
$G_{J}(X_J)$ die gemeinsame Wahrscheinlichkeitsverteilung der Variablen über die Indexmenge $J$.
\\
Ist die Anzahl $n$ der Argumente von g klein, so nutze eine alternative Kurzschreibweise für die Marginalfunktionen. 
Schreibe statt einem Argument $x_i$ von $g$ ein $+$ um anzudeuten, dass über diese Variable summiert wird.

\subsection{Funktionsweise des Algorithmus}

Der Sum-product Algorithmus operiert mithilfe einer "message passing" Porzedur, die Produukte von lokalen 
Funktionen entlang der Pfade des Faktorgraphen aufsammelt. Es wird angenommen,dass dieser Graph ein Baum ist, d.h.
dass dieser Graph keine Kreise enthält. Die Beschreibung des Algorithmus kann durch die Annahme vereinfacht werden, dass jeder Knoten wie ein
Prozessor Nachrichten über Kanten übermittelt und empfängt. \\

Für diese vereinfachte Betrachtung arbeitet der Algorithmus wie folgt. Die Basisoperation an jedem Knoten ermittlet
das Produkt aller eingehenden Nachrichten an diesem Knoten. Für Knoten, die eine Menge von Variablen darstellen, wird dieses Produkt
um die zugehörigen lokalen Funktionen erweitert. Die so ermittelten Produkte werden dann mit dem Vorbehalt, dass
Nachrichten über ausgehende Knoten keine Faktoren enthalten, über die ausgehenden Kanten übermittelt. Da vorrausgesetzt
wurde, dass der Faktorgraph keine Kreise enthält, enthält das Produkt der ausgehenden Nachrichten einer Kante und der empfangenen
Nachrichten dieser Kante alle Faktoren der globalen Funktion. \\

Diese sogenannte "message-passing" Prozedur wird von den Blättern des Faktorgraphs aus gestartet und iteriert über alle
Knoten des Graphen. An Blättern, die Variablenmengen darstellen entspricht die ausgehende Nachricht einer Representation der lokalen 
Funktion dieses Knotens. An Blättern, die eine Variable darstellen entspricht sie hingegen der Indikatorfunktion.
Alle anderen Knoten des Graphen warten zunächst bis sie genügend Nachrichten gesammelt haben um eine ausgehende 
Nachricht zu produzieren. Genauer soll das heißen, sie warten solange, bis an jeder bis auf einer eingehenden Kante Nachrichten
empfangen wurden. Tritt dieser Fall ein, so wird das Produkt aller eingehenden Nachrichten mit der lokalen Funktion gebildet und über die freie Kante übermittelt.
Wird an dieser übrig gebliebenen Kante eine Nachricht empfangen, so werden die Produkte der zugehörigen lokalen Funktion über alle
anderen ausgehnden Kanten versendet. Diese Prozedur wird in Figur \textbf{FIGUR EINFÜGEN!!!} illustriert.\\

Dieser Algorithmus wird dann effektiv, wenn man beachtet, dass von lokalen Funktionen über Pfade gesammelte Produkte
marginalisiert werden können. D.h., dass nicht alle Variablen entlang einer Kante beachtet werden müssen. Im Allgemeinen
muss eine Variable beachtet werden, wenn sie Argument einer nachfolgenden lokalen Funktion ist. Andernfalls kann
sie vernachlässigt werden. \\

Figur \textbf{FIGUR EINFÜGEN!!!} zeigt das Fragment eines Faktorgraphen. Die Update Regeln für dieses Fragment ergeben sich dann also

\begin{equation}
 \mu_{x \rightarrow A}(x) = \mu_{B \rightarrow x}(x) \cdot \mu_{C \rightarrow x}(x)  
\end{equation}

\begin{equation}
 \mu_{A \rightarrow x}(x) = \sum_{y,z} f_A(x,y,z) \cdot \mu_{y \rightarrow A}(x) \cdot \mu_{z \rightarrow A}(x)
\end{equation}

\begin{equation}
 F_x(x) = \mu_{x \rightarrow A}(x) \cdot \mu_{A \rightarrow x}(x)
\end{equation}

\subsection{\textit{Belief Propagation} in Baysschen Netzwerken}

\text{FIGUR EINFÜGEN!!!}\\
Die Verteilungsfunktion () eines Bayssschen Netzwerkes erlaubt eine intuitive Umformulierung zur Representation
eines Faktorgraphen. Eine zu einem einzigen Faktor gehörende lokale Funktion in (), hat die Form $f(x|a(x))$, wobei $a(x)$ die Menge der
Nachfolger von x im zugehörigen Baysschen Netzwerk ist. In Faktorgraphen wurden Nachfolgerknoten durch Pfeile gekennzeichnet.
Diese Pfeile erlauben es uns einen Faktorgraphen ebenso als Bayssches Netzwerk ansehen zu können.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newpage  % neuer Abschnitt auf neue Seite, kann auch entfallen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newpage  % neuer Abschnitt auf neue Seite, kann auch entfallen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


  
\section{Hochdimensionale Integration}

In diesem Kapitel stellen wir verschiedene Integrationstechniken zur Approximation Hochdimensionaler Integrale

\begin{equation}
 \int_{\Omega} f(\mathbf{x}) d \mathbf{x} \approx \mathcal{Q}f = \sum_{i=1}^N \omega_i f(x_{(i)})
\end{equation}

vor. Wir bezeichnen $\Omega$ als das d-dimensionale Integrationsgebiet und $\mathbf{x_{(i)} \in \Omega$ als Stützstellen
bzw. $\omega_i$ als Gewichte des Quadraturverfahrens $\mathcal{Q}$. Alle hier vorgestellten Quadraturverfahren 
lassen sich als Spezialfälle dieser allgemeinen Darstellung auffassen und unterscheiden sich durch die jeweilige Wahl der
Stützstellen und Gewichte. \\
Wie auch bei anderen Approximationsverfahren interessieren wir uns für ein Maß der Approximationsgüte einzelner
Verfahren. Deshalb werden wir in diesem Kapitel neben der Beschreibung auch auf die Konvergenzeigenschaften 
der Verfahren eingehen. Für Monte Carlo Verfahren lässt sich bespielsweise eine dimensionsunabhängige Konvergenzrate
zeigen, während die Konvergenzrate für die anderen Verfahren (Quasi-Monte Carlo und Dünne Gitter) stark von der effektiven
Dimension des Problems abhängt. \\
Als absoluten Fehler für die Approximationsgüte in unserer Analyse wählen wir 

\begin{equation}
 \epsilon(f) := |Qf - \int_\Omega f(\mathbf{x}) d\mathbf{x}| .
\end{equation}

\subsection{Monte Carlo}

Bei der Monte Carlo Integration wählen wir die Stützstellen $\mathcal{x_{(i)}}$ als Zufallszahlen einer d-dimensionalen
Gleichverteilung und feste Gewichte $\omega_i = \frac{1}{N}$. 
Das so entstandene Verfahren 

 \begin{equation}
 \int_{\Omega} f(\mathbf{x}) d \mathbf{x} \approx \frac{1}{N} \sum_{i = 1}^N f(x_{(i)}) \text{ mit } x_{(i)} \sim \mathcal{U_d}(\Omega)
 \end{equation}

 hat nach dem Gesetz der Großen Zahlen für beschränkte Varianzen eine Konvergenzrate von 
 
 \begin{equation}
  \epsilon(f) = \mathcal{O}(N^{-\frac{1}{2}}).
 \end{equation}
 
 Wir sehen schnell, dass der Nachteil dieses Verfahrens in seiner geringen Konvergenzrate für niedrige Dimensionen
 liegt. Vorteile sind die geringen Vorraussetzungen an den Integranden und die Dimensionsunabhängigkeit der
 Konvergenzrate. 
 
 \subsection{Quasi-Monte Carlo}
 
 Bei Quasi-Monte Carlo Verfahren verwenden wir statt Pseudo Zufallszahlen als Auswertungspunkte 
 deterministische Punktfolgen so. Nieder-Diskrepanz-Folgen, die das Integrationsgebiet $\Omega$
 überdecken. In dieser Arbeit haben wir Sobolev-, Niederreiter- und Ritchmeyer-Folgen () implementiert. 
 Diese Änderung führt dazu, dass die Integrationspunkte gleichmäßiger auf dem Integrationsgebiet verteilt sind.
 Dies wird in Abbildung () deutlich. 
 
 Die verbesserten Verteilungseigenschaften liefern im geringen und mittleren Dimensionsbereich eine Verbesserung 
 der Konvergenzrate auffassen
 
 \begin{equation}
  \mathcal{O}(\frac{\log(N)^d}{N}).
 \end{equation}
 
 \subsection{Produktansatz}
 
 Der Produktansatz basiert auf eindimensionalen Quadraturverfahren. Ein eindimensionales Quadraturverfahren ist durch
 \begin{equation}
  Q_l^{(1)}f := \sum_{i = 1}^{N_l} \omega_i f(x_{li}) 
 \end{equation}
 
 mit Diskretisierungslevel l definiert. \\
 Durch Tensorierung von eindimensionalen numerischen Quadraturformeln können wir eine Quadraturformel 
 für hochdimensionale Integrale konstruieren. 
 Sei $l_i$ das Diskretisierungslevel der Quadraturformel in der i-ten Dimension und $N_{li}$ die zugehörige 
 Anzahl von Stützstellen, so lässt sich das Tensorprodukt der Quadraturformeln mit den $i_l$-ten Stützstellen und den $i_l$-ten Gewichten durch 
 
 \begin{equation}
  \begin{split}
  Q^{(d)}f &=(Q_{l_1}^{(1)} \otimes ... \otimes Q_{l_d}^{(1)})f \\
  &= \sum_{i_1 = 1}^{N_{l_1}} ... \sum_{i_d = 1}^{N_{l_d}} \omega_{l_1} \cdot ... \omega_{l_d} \cdot f(x_{l_1 i_1},...,x_{l_d i_d}) \\
  \end{split}
 \end{equation}
 
 definieren. \\
 
 Ein Nachteil dieses Ansatzes ist die exponentiell wachsende Anzahl von Auswertungspunkten (Fluch der Dimension):
 Ein Quadraturverfahren mit der gleichen Diskretisierung in jede Richtung, also mit $N:= N_{l_1} = ... = N_{l_d}$-Auswertungspunkten,
 benötigt in $d$ Dimensionen ingesamt $N^d$ Auswertungspunkte. \\
 
 Wir erhalten als Konvergenzrate des Verfahrens
 
 \begin{equation}
  \epsilon(f) = \mathcal(O)(n^{-\frac{\alpha}{d}})
 \end{equation}
 
 Hierbei beschreibt $\alpha$ die Konvergenzrate des eindimensionalen Quadraturverfahrens.
 Aufgrund der exponentiell von der Dimension $d$ abhängenden Konvergenzrate ist dieses Verfahren
 für hohe Dimensionen unbrauchbar.
 
 \subsection{Eindimensionale Quadraturverfahren}
 
 Wir beschränken uns in dieser Arbeit zunächst auf die Clenshaw-Curtis Regel aus ().
 Neben den Gauß-Quadraturformeln ist diese eine der bedeutensten eindimensionalen Quadraturregeln und
 für die in Kapitel () auftretenden Integrale gut geeignet. 
 
 \subsubsection{Clenshaw-Curtis}
 
 Auch wenn die Clenshaw-Curtis Regel keinen optimalen Exaktheitsgrad hat, ist sie aufgrund ihrer
 geschachtelten Struktur der Gewichte und Stützstellen für die Integrations mittels dünner Gitter 
 sehr hilfreich. \\
 Die Stützstellen und Gewichte der offenen Clenshaw-Curtis-Formeln ergeben sich als Extrempunkte der 
 Chebycheff-Polynome auf $(0,1)$
 
 \begin{equation}
  \begin{split}
      x_{l_i} &= \frac{1}{2}(1- \cos(\frac{\pi i}{N_l +1})) \\
      w_{l_i} &= \frac{2}{N_l +1} \sin{(\frac{\pi i}{N_l +1})} \sum_{j = 1}^{\frac{(N_l + 1)}{2}} \frac{1}{2j -1} \sin{(\frac{(2j -1) \pi i}{N_l +1})}.
    \end{split}
 \end{equation}

 Der Index $l$ gibt das Level und $N_l := 2^l -1$ die Anzahl der Stützstellen bzw. der Gewichte an.
 
 \subsection{Dünne Gitter}
 
 Ein Ansatz dem eben erwähnten Fluch der Dimension zu entgehen bieten die sogenannten Dünnen Gitter. 
 Damit kann erreicht werden, dass die Konvergenzrate, ähnlich zu Quasi-Monte Carlo Techniken, nur
 logarithmisch von der Dimension des Problems abhängt. \\
 
 Wir wählen den Ansatz mittels Differenz von zwei Quadraturformeln unterschiedlicher Level. Sei eine 
 Folge von Quadraturformeln mit $N_l^{(d)}$ Auswertungspunkten für ein Level $l \in \N$ mit 
 $ N_l^{(d)} < N_{l+1}^{(d)}$ gegeben. \\ 
 Wir definieren die Differenz zwischen zwei Leveln durch 
 
 \begin{equation}
 \begin{split}
  \Delta_k^{(1)}f &= (Q_k^{(1)} - Q_{k-1}^{(1)})f \\ 
  &= \sum_{i=1}^{N_k} \omega_{k,i} f(x_{k,i}) - \sum_{i=1}^{N_k} \omega_{k-1,i} f(x_{k-1,i}) \\
  \text{ und } Q_0 f := 0.
  \end{split}
 \end{equation}


  Mittels dieser Differenzen lässt sich die klassische Dünngitter-Quadratur nach Smolyak (siehe) mit $k \in \N^d$
  als 
  
  \begin{equation}
   Q_l^{(d)}f = \sum_{k_1 + ... + k_d \leq l+d-1} (\Delta_{k_1}^{(1)} \otimes ... \otimes \Delta_{k_d}^{(1)}) f
  \end{equation}
  
  definieren. Anders als beim Produktansatz werden in der Dünngitter-Quadratur nur die Summanden betrachtet, 
  deren Summe der Level echt kleiner als die vorgebene Konstante $l+d$ ist. 
  
  Für geschachtelte Quadraturformeln, wie die vorgestellte Clenshaw-Curtis-Regel, lässt sich die Teleskopsumme in ()
  zu 
  
  \begin{equation}
   \sum_{i=1}^{N_k} {\omega}_{k,i}f(x_{k,i}) \text{ mit } {\omega}_{k,i} 
   =  \begin{cases} \omega_{k,i}&\text{falls i ungerade}\\ \omega_{k,i} - \omega_{k-1,\frac{i}{2}}&\text{falls i gerade }\end{cases}
  \end{equation}
  
  vereinfachen. \\
  
  Zur Fehleranalyse dieses Verfahrens betrachten wir die Klasse $\mathcal{W}_d^r$ der Funktionen mit beschränkten
  Ableitungen bis zur Ordnung $r$:
  
  \begin{equation}
   \mathcal{W}_d^r := \{g: \Omega \rightarrow \R, ||\frac{\delta^{|s|}g}{\delta^{|s_1|} x_1,...,\delta^{|s_d|} x_d}||_{\infty} < \infty, s_i \leq r\}.
  \end{equation}
  
  Erfüllt die eindimensionale Quadraturregel zusätzlich 
  
  \begin{equation}
   \epsilon(f) = \mathcal{O}(n_l^{-r})
  \end{equation}

  und gilt $N_l = \mathcal{O}(2^l)$, dann ergibt sich für alle Funktionen $d \in \mathcal{W}_d^r$
  die Fehlerabschätzung
  
  \begin{equation}
   \epsilon(f) = \mathcal{O}(N_l^{-r}(\log N_l)^{(d-1)(r+1)})
  \end{equation}

  für die Integration mittels dünner Gitter. Der Unterschied der dünnen Gitter für die Clenshaw-Curtis-Regel
  gegenüber dem Produktansatz ist anschaulich in Abbildung () zu erkennen.
  
  \section{Algorithmen in Baysschen Netzen}
  
  In diesem Kapitel betrachten wir Algorithmen zur Berechnung von Posterior Wahrscheinlichkeit in Baysschen Netzen.
  Um Erwartungen und Posterior Wahrscheinlichkeiten zu bestimmen sind schnelle Methoden zur nunerischen Integration notwendig. 
  In Kapitel () stellen wir bekannte Methoden, wie die von Genz () vor. Hier konzentrieren wir uns auf eine neue deterministische
  Approximationsmethode, welche eine höhere Genauigkeit als viele der üblichen Verfahren (Monte-Carlo- und Quasi-Monte-Carlo-Verfahren) bei gleichbleibenden Kosten 
  erreicht. \\
  Wir gehen davon aus, dass wir eine Menge von Daten gegeben haben, aber an einer Unbekannten $x$ interessiert sind.
  Schlussfolgerungen über diese Unbekannte $x$ können wir mittels der Posterior Wahrscheinlichkeit: 
  
  \begin{equation}
   P(x|D) = \frac{P(x,D)}{P(D)} = \frac{P(x,D)}{\int_x P(x,D) dx}
  \end{equation}
  
  ,wobei D die beobachtete Datenmenge bezeichnet. 
  
  Wie auch im Kapitel () über das \textit{Trueskill} Verfahren deutlich wird, sind wir daran interessiert
  die Posterior Erwartung und Varianz der Unbekannten zu bestimmen. Diese ergeben sich als Integrale 
  über den Posterior: 
  
  \begin{equation}
  \begin{split}
   E[x|D] &= \int_x xP(x|D) dx = \frac{\int_x x P(x,D) dx}{\int_x P(x,D) dx } \\
   E[x^2|D] &= \int_x x^2P(x|D) dx = \frac{\int_x x^2 P(x,D) dx}{\int_x P(x,D) dx } \\
   Var[x|D] &= E[x^2|D] - E[x|D]^2 \\
  \end{split}
  \end{equation}
  
  Ist die Unbekannte, die uns interessiert nicht die einzige Unbekannte des Systems, so müssen weitere Unbekannte
  aus der gemeinsamen Verteilung heraus marginalisiert werden. Das Problem erweitert sich dann um einige Integrale: 
  
  \begin{equation}
   P(x|D) = \frac{\int_{y,z} P(x,y,z,D)}{\int_{x,y,z} P(x,y,z,D)}
  \end{equation}

  Wir unterscheiden bei klassischen numerischen Integrationsmethoden zwischen deterministischen und nichtdeterministischen Methoden. 
  Deterministische Methoden versuchen den Integranden durch einen Ausdruck zu approximieren, dessen Integral
  exakt berechnet werden kann. Nichtdeterministische Methoden werten den Integranden an Zufallspunkten aus, um
  so den Wert des Integrals zu schätzen. Deshalb lassen sich Nichtdeterministische Methoden auf eine größere Klasse von Problemen 
  anwenden. Sie brauchen jedoch mehr Rechenzeit als deterministische Methoden. \\
  Die hier vorgestellte Methode, Expectation Propagation, ist eine deterministische Approximationsmethode. 
  Sie ist eine Erweiterung des \textit{assumed-densitiy Filtering}-Verfahrens (Maybeck 1982).
  In ADF werden Beobachtungen Schritt für Schritt betrachtet und der Posterior mittels Minimierung der KL-Divergenz approximiert.
  Expectation Propagation (EP) erweitert das ADF Framework um einige zusätzliche Schritte, welche eine 
  iterative Verfeinerung der Approximation bewirken. Dabei verfeinern Informationen von spätereren
  Beobachtungen Approximationen, die früher gemacht wurden. Sobald die Verfeinerung konvergiert, sind der resultierende
  Posterior unabhängig von der Reihenfolge der Beobachtungen.
  Die Kosten dieses Verfahrens vergrößern sich dadurch nur um einen konstanten Faktor. 
  
  \subsection{ADF Methode}
  
  Die Hauptidee der ADF Technik ist es, die KL-Divergenz zwischen exaktem Posterior und er Approximation zu minimieren.
  \textit{Assumed-density Filtering (ADF)} ist eine grundlegende Technik um Posteriori in Bayschen Netzwerken
  und anderen statistischen Modellen zu berechnen. \\
  Sei $\Theta$ ein ein d-dimensionaler, gaußverteilter Vektor mit Verteilung 
  \begin{equation}
   P(\Theta) \sim \mathcal{N}(0,100Id)
  \end{equation}
  
  Die gemeinsame Verteilung von $\Theta$ und $n$ unabhängigen Beobachtungen $D = \{x_1,...,x_n\}$ ist dann: 
  
  \begin{equation}
   P(D|\Theta) = P(\Theta) \prod_i P(x_i|\Theta).
  \end{equation}
  
  Figur einfügen \\
  
  Zunächst schreiben wir die gemeinsame Verteilung $P(D, \Theta)$ als ein Produkt von Termen
  $t_i$: 
  
  \begin{equation}
   P(D, \Theta) = \prod_i t_i(\Theta).
  \end{equation}
  
  Für klassiusche Bayssche Netzwerke, die auch im \textit{Trueskill} Verfahren auftreten wählen wir eine 
  Faktorisierung mittels bedingter Wahrscheinlichkeiten: $\prod_{nodes} P(Y|pa(Y))$, wobei $pa(Y)$
  die Eltern von Knoten $Y$ im zugehörigen Graphen beschreibt. \\
  Im zweiten Schritt wählen wir eine parametrische Approximationsverteilung. Dabei ist es von großer Bedeutung, 
  dass dieser Verteilung der Familie der exponentiellen Verteilungen angehört, sodass nur eine feste Anzahl 
  von Erwartungen berechnet werden muss. 
  Normalerweise schränkt der Definitionsraum und die Eigenschaften von $\Theta$ diese Verteilung soweit ein, dass 
  diese nahezu eindeutig bestimmt ist. 
  Für Bayssche Netzwerke schlagen wir eine spherische Gaußverteilung der Form
  
  \begin{equation}
   q(\Theta) \sim \mathcal{N}(\mathbf{m}_{\Theta})
  \end{equation}

  vor. 
  
  Zum Schluss bringen wir die Terme $t_i$ in eine Reihenfolge und gliedern sie in den approximierten 
  Posterior ein. Dies ist in Abbildung () anschaulich dargestellt. Wir starten mit $q(\Theta) - 1$. Den Prior 
  Term einzugliedern ist dann trivial und benötigt keine Approximation. Um den nächsten Term $t_i(\Theta)$ einzugliedern, 
  wählen wir den exakten Posterior
  
  \begin{equation}
   P(\Theta) = \frac{t_i{\Theta}q{\Theta}}{\int_{\Theta}t_i(\Theta)q(\Theta)d \Theta}
  \end{equation}
   
  und minimieren die KL-Divergenz $D(P(\Theta)||q^{new}(\Theta)) d \Theta$ unter der Annahme, dass
  $q^{new}(\Theta)$ gaußverteilt ist. Die Nullbedingung an den Gradienten von $(\mathbf{m}_\Theta, \nu_{\Theta})$
  erzwingt die Gleichungen
  
  \begin{equation}
  \begin{split}
   \mathbf{m}_{\Theta}^{new} &= \int_{\Theta} P(\Theta) \Theta d \Theta \\
   \nu_{\Theta}^{new} d + (\mathbf{m_{\Theta}^{new}}^T \mathbf{m_{\Theta}}^{new}) &= \int_{\Theta} P(\Theta) \Theta^T \Theta d \Theta \\
  \end{split}
  \end{equation}
   
  Oder als Erwartungswerte geschrieben 
  
  \begin{equation}
   E_{q^{new}}[\Theta] = E_P[\Theta]
   E_{q^{new}}[\Theta^T \Theta] = E_{P}[\Theta^T \Theta]
  \end{equation}

  Um diese Erwartungen berechnen zu können sind die folgenden Beziehungen sehr hilfreich: 
  
  \begin{equation}
  \begin{split}
   Z(\mathbf{m}_{\Theta} \nu_{\Theta}) &= \int_{\Theta} t(\Theta) q(\Theta) d \Theta \\ 
   &= \int_{\Theta} \frac{t(\Theta)}{(2 \pi \nu_{\Theta})^{d/2}} exp(- \frac{1}{2 \nu_{\Theta}}(\Theta - \mathbf{m}_{\Theta})^T (\Theta - \mathbf{m}_{\Theta})) d \Theta \\
   \nabla_m \log Z(\mathbf{m}_{\Theta}, \nu_{\Theta}) & = \frac{1}{Z} \int_{\Theta} \frac{(\Theta - \mathbf{m}_{\Theta})}{\nu_{\Theta}} \frac{t(\Theta)}{(2 \pi \nu_{\Theta})^{d/2}} exp(- \frac{1}{2 \nu_{\Theta}}(\Theta - \mathbf{m}_{\Theta})^T (\Theta - \mathbf{m}_{\Theta})) d \Theta \\
   &= \frac{E_P[\Theta]}{\nu_{\Theta}} - \frac{\mathbf{m}_{\Theta}}{\nu_{\Theta}} \\
   E_P[\Theta] &= \mathbf{m}_{\Theta} + \nu_{\Theta} \nabla_m \log Z(\mathbf{m}_{\Theta},\nu_{\Theta}) \\
   E_P[\Theta^T \Theta] - E[\Theta]^TE[\Theta] &= \nu_{\Theta} d - \nu_{\Theta}^2 (\nabla_m^T \nabla_m - 2 \nabla_{\nu} \log Z(\mathbf{m}_{\Theta}, \nu_{\Theta}) ) \\ 
  \end{split}
  \end{equation}

  Diese Gleichungen beschreiben Eigenschaften der Gaußverteilung und gelten für beliebige $t(\Theta)$
  
  Der endgültige ADF Algorithmus ergibt sich dann als: 
  
  \begin{enumerate}
   \item Initialisiere $\mathbf{m}_{\Theta} = 0, \nu_{\Theta} = 100 \text{ Prior }, s = 1 (\text{ Skalierungsfaktor })$
   \item Für jeden Datenpunkt $\mathbf{x_i}$, aktualisiere $(\mathbf{m_{\Theta}}, \nu_{\Theta},s)$ nach den Regeln
   \begin{equation}
    \begin{split}
      \mathbf{m}_{\Theta}^{new} &= \mathbf{m}_{\Theta} + \nu_{\Theta} r_i \frac{\mathbf{x_i - m_{\nu}}}{\nu_{\Theta} + 1} \\
      \nu_{\Theta}^{new} &= \nu_{\Theta} -  r_i (1 - r_i) \frac{(\mathbf{x_i - m_{\Theta}})^T (\mathbf{x_i - m_{\Theta}})}{d(v_{\nu} +1)^2} \\
      s^{new} &= s x \mathbf{Z_i}(\mathbf{m_{\Theta}}, \nu_{\Theta}) \\
    \end{split}
   \end{equation}
  \end{enumerate}

  Für jeden Datenpunkt berechnen wir die Wahrscheinlichkeit, dass dieser nicht in Unordnung ist, machen
  eine Aktualisierung unserer Vermutung an $\Theta$ $(\mathbf{m}_{\Theta})$ und ändern unsere Zuversicht an unsere
  Schätzung $(\nu_{\Theta})$. \\
  Wir sehen, dass der Algorithmus von der Reihenfolge der Betrachtung der Datenpunkte abhängt. Dies liegt daran, dass
  die Wahrscheinlichkeit eines Datenpunktes in Unordung zu sein von der aktuellen Vermutung an $\Theta$ abhängt. 
  Genauere Ergebnisse erhalten wir, wenn wir statt spherischen Gaußverteilungen, Gaußverteilungen mit voller
  Kovarianzmatrix betrachten. In diesem Fall werden deutlich mehr Erwartungen vorberechnet, was die Genauigkeit 
  dieses Verfahrens positiv beeinflusst. 
  Abbildung () zeigt die Ausgabe des Algorithmus für drei verschiedene, zufällig gewählte Sortierungen 
  der Datenpunkte. 
  Wir sehen, dass der Fehler immer dann stark ansteigt, wenn gleiche Datenpunkte zusammen berechnet werden.
  Außerdem erkenn wir, dass insebsondere das Verarbeiten der Daten in sortierter Reihenfolge schlecht ist. 
  Dies liegt daran, dass der Algorithmus sich dann zu sehr auf eine Eingabe konzentriert, bevor er merkt, dass weitere
  Input Daten verfügbar sind und diese somit vernachlässigt. Darunter leiden dann Offensichtlich die approximierten Werte
  für die Erwartung und die Varianz. 
  Eine Möglichkeit ist es, zuerst eine Ordnung der Daten zu finden, die die natürlichen Abweichungen in den Daten 
  am besten wiederspiegelt. Eine andere Möglichkeit ist es die Abhängigkeiten von der Reihenfolge der Daten zu eliminieren. 
  Diese Methode bschreiben wir im nächsten Kapitel. 
  
  \subsection{EP Methode}
  
  In diesem Abschnitt beschreiben wir den EP-Algorithmus und beschreiben sein Anwendung beispielhaft für 
  das Clutter Problem. Der EP-Algorithmus basiert auf dem im vorherigen Abschnitt vorgestellten \textit{assumed-density filtering}-Verfahren.
  Im ADF- Algorithmus behandeln wir jeden Term $t_i$ als exakt und approximieren dann den Posterior mittels $t_i$. 
  Eine etwas andere Interpretation wäre erst $t_i$ durch einen Term $t_i'$ zu approximieren, um dann einen exakten
  Posterior mit $t_i'$ weiter zu benutzen. Diese Interpretation ist ebenfalls zulässig, da wir $t_i'$ als Ratio des neuen 
  Posterior durch 
  
  \begin{equation}
   t'(\Theta) = Z \frac{q^{new}(\Theta)}{q(\Theta)}
  \end{equation}
  
  definieren können. Multiplizieren wir diesen Term mit $q(\Theta)$, so erhalten wir wie gewünscht $q^{new}(\Theta)$.
  Eine wichtige Eigenschaft ist es, dass der approximierte Posterior wiederum gaußverteilt ist. \\
  Der Algorithmus des vorherigen Kapitel berechnet also nacheinander eine Gaußsch Approximation $t_i'(\Theta)$ für jeden 
  Term $t_i(\Theta)$ und fügt diese Aprroximationen zu einem neuen gaußschen Posterior für $\Theta$ zusammen.
  Unter dieser Betrachtungsweise haben die Approximationen keine zu beachtende Reihenfolge. Denn wir können jederzeit
  einen Schritt zurückgehen und unsere früheren Approximationen verfeinern. Mit dieser Betrachtungsweise können wir den EP-Algorithmus
  formulieren. \\
  Aus dem vorherigen Abschnitt erhalten wir die folgende Approximation für einen Term: 
  
  \begin{equation}
  \begin{split}
   \mathbf{Z}(\mathbf{m_{\Theta}}, \nu_{\Theta}) &= \int_{\Theta} t_i(\Theta) q(\Theta) d \Theta \\
   t_i'(\Theta) &= \mathbf{Z_i}(\mathbf{m_{\Theta}}, \nu_{\Theta}) \frac{q^{new}(\Theta)}{q(\Theta)} \\
   &= \mathbf{Z_i}(\mathbf{m_{\Theta}}, \nu_{\Theta}) (\frac{\nu_{\Theta}}{\nu_{\Theta}^{new}})^{d/2} exp(-\frac{1}{\nu_{\Theta}^{new}} (\Theta - \mathbf{m}_{\Theta}^{new})^T (\Theta - \mathbf{m}_{\Theta}^{new}))\\
   &exp( \frac{1}{2 \nu_{\Theta}} (\Theta - \mathbf{m}_{\Theta}^{new})^T (\Theta - \mathbf{m}_{\Theta}^{new})) \\
   \end{split}
  \end{equation}

  Wir definieren nun $(\mathbf{m}_i, \nu_i)$ durch:
  
  \begin{equation}
  \begin{split}
   \nu_i^{-1} &= (\nu_{\Theta}^{new})^{-1} - \nu_{\Theta}^{-1} \\
   \mathbf{m}_i &= \nu_i(\nu_{\Theta}^{new})^{-1} \mathbf{m}_{\Theta}^{new} - \nu_i \nu_{\Theta}^{-1} \mathbf{m}_{\Theta} \\
   &= \mathbf{m}_{\Theta} + (\nu_i - \nu_{\Theta}) \nu_{\Theta}^{-1}(\mathbf{m}_{\Theta}^{new} - \mathbf{m}_{\Theta}). \\
  \end{split}
  \end{equation}

  Damit erhalten wir für die Terme $t_i'(\Theta)$ die einfachere Form: 
  
  \begin{equation}
   t_i'(\Theta) = \frac{\mathbf{Z}_i(\mathbf{m}_{\Theta}, \nu_{\Theta})}{\mathcal{N}(\mathbf{m}_i; \mathbf{m}_{\Theta}, (\nu_i + \nu_{\Theta})^T)} \mathcal{N}(\Theta; \mathbf{m}_i, \nu_i^T).
  \end{equation}

  Nach Konstruktion erhalten wir das ADF Update mit Skalierungsfaktor $\mathbf{Z}_i$, wenn wir diesen Ausdruck mit 
  $q(\Theta)$ multiplizieren. \\
  Abbildung () illustriert $t_i(\Theta)$ und $t_i'(\Theta)$ für das Clutter Problem. \\
  
  Bis hierhin haben wir nur die ADF Aktualisierungen in einer anderen Art und Weise formuliert. 
  Um den EP Algorithmus zu erhalten verfeinern wir alle Termvariablen $(\mathbf{m}_i, \nu_i)$ basierend auf allen anderen Approximationen. 
  Der allgemeine EP Algorithmus lautet: 
  
  \begin{enumerate}
   \item Initialisiere die Term Approximationen $t_i'$
   \item Berechne den Posterior für $\Theta$ mithilfe des Produktes vom $t_i'$:
   \begin{equation}
    q^{new}(\Theta) = \frac{\prod_i t_i'(\Theta)}{\int \prod_i t_i'(\Theta) d \Theta}
   \end{equation}
   \item Bis alle $t_i'$ konvergieren: 
   \begin{enumerate}
    \item Wähle ein $t_i'$.
    \item Entferne $t_i'$ aus dem Posterior, um einen früheren Posterior $q(\Theta)$ zu erhalten
    \begin{equation}
     q(\Theta) \propto \frac{q^{new}(\Theta)}{t_i'(\Theta)}
    \end{equation}
    \item Berechne den Posterior $q^{new}(\Theta)$ und den Normalisierungsfaktor $Z_i$ aus $q(\Theta)$ 
    und $t_i(\Theta)$ mittels ADF.
    \item Setze 
    \begin{equation}
     t_i' = Z_i \frac{q^{new}(\Theta)}{q(\Theta)}
    \end{equation}
   \end{enumerate}
   \item Nutze die Normalisierungskonstante von $q^{new}(\Theta)$ (aus 1.) als eine Approximation für $P(D)$
   \begin{equation}
    P(D) \approx \int \prod_i t_i'(\Theta) d \Theta
   \end{equation}
  \end{enumerate}

  Sobald das Verfahren konvergiert ist das Ergebnis unabhängig von der Reihenfolge der Betrachtung der Terme $t_i$.
  In der Beschreibung des Algorithmus haben wir die Division verwendet um $t_i'$ aus dem Posterior zu entfernen. 
  Wir können stattdessen aber auch alle Terme außer $t_i'$ multiplizieren 
  
  \begin{equation}
   q(\Theta) \propto \prod_(j \neq i) t_j'(\Theta).
  \end{equation}
  
  \subsection{Anwendung auf das Clutter Problem}
  
 \section{Trainingingsmethoden}
 
 In diesem Kapitel beschreiben wir eine Methode, wie der Expectation Algorithmus mit der \textit{Bayes-Point-Machine}
 verknüpft werden kann. Die \textit{Bayes-Point-Machine} ist eine Bayssche Spezifikation der \textit{Support-Vector-Machine (SVM)}. 
 Die \textit{Bayes-Point-Machine} approximiert den Baysschen Durchschnitt mithilfe des sogenannten \textit{Bayes Punktes}.
 Diesen \textit{Bayes Punkt} zu finden, ist eine einfachere, aber immernoch sehr komplizierte Aufgabe. 
 \\
 Wir zeigen hier, dass Expectation Propagation mit für einen durch eine Kovarianzmatrix beschriebenen, gaußverteilten
 Posterior eine vollständige Approximation des Bayschen Durchschittes erzeugt. 
 
 \subsection{Die \textit{Bayes-Point-Machine} }
 
 Die \textit{Bayes-Point-Machine (BPM)} ist eine Bayssche Erweiterung von linearen Klassifizierern. 
 Ein linearer Klassifizierer, klassifiziert einen Punkt $\mathbf{x}$ mittels eines Vektors $mathbf{w}$
 nach $y = sign(\mathbf{w}^T \mathbf{x})$. Sei eine Trainingsmenge $D = \{(\mathbf{x_1}, y_1),...,(\mathbf{x_n}, y_n)\}$ gegeben, so 
 können wir den Likelyhood für $\mathbf{w}$ schreiben als: 
 
 \begin{equation}
  P(D|\mathbf{w}) = \prod_i P(y_i| \mathbf{x_i} \mathbf{w}) = \prod_i \Theta(y_i \mathbf{w}^T \mathbf{x_i}) \\
  \Theta(z) = \begin{cases}1&\text{falls }z > 0\\1&\text{falls }z \leq 0\end{cases} 
 \end{equation}
 
 Wir möchten hier anmerken, dass $P(D|\mathbf{w})$ nur Verteilung von $y$ ist, und nicht von $\mathbf{x}$.
 Dieser Likelyhood ist 1, falls $\mathbf{w}$ perfekter Separierer ist und 0 sonst. Lineare Separation kann
 leicht durch stufenweise Vergrößerung der Menge an Eigenschaften erzielt werden. 
 Zum Beispiel in dem man dem urpsrünglichen Raum Quadrate hinzufügt, bis die Daten in dem so neu erzeugten Raum
 separabel sind. Wir erzeugen so einen nichlinearen Entscheidungsrand im urprünglichen Raum. 
 Eine solche Vergrößerung der Menge an Eigenschaften kann leicht mittels des sogenannten \textit{Kernel tricks} 
 effizient implementiert werden. Dabei schreiben wir den Algorithmus mithilfe von inneren Produkten um, und 
 erreichen somit eine Vergrößerung der Menge an Eigenschaften mittels dieser inneren Produkte. 
 Diese Methode wollen wir in einem späteren Abschnitt beschreiben. 
 Weitere Verfeinerungen der Methode können in (Minka 2001...) nachgelesen werden. Minka deutet zum Beispiel an, wie Unsicherheiten
 in den Trainingsdaten berücksichtigt werden können. \\
 
 Um unser Bayssches Modell zu vervollständigen müssen wir den Prior auf $\mathbf{w}$ spezifizieren. 
 Auch hier möchten wir für Details auf Minka ... verweisen. 
 Wir wählen für $\mathbf{w}$ eine spärische Gaußverteilung
 
 \begin{equation}
  P(\mathbf{w}) \sim \mathcal{N}(\mathbf{0,I}).
 \end{equation}

 Damit folgt aus den Eigenschaften der spärischen Gaußverteilung, dass $\mathbf{w}$ unabhängig vom Radius $r$
 gleichverteilt auf der Kugel vom Radius r ist. Dieser Prior ist somit unabhängig vom Entscheidungsrand.
 \\
 In diesem Modell lässt sich ein neuer Datenpunkt $\mathbf{x}$, dann durch 
 
 \begin{equation}
  P(y|\mathbf{x},D) = \int_{\mathbf{w}} P(y|\mathbf{x,w}) P(\mathbf{w}|D) d \mathbf{w}
 \end{equation}
 
 klassifizieren. Wir sehen, dass somit in jedem Schritt ein Integral gelöst werden muss. 
 Eine Möglichkeit ist es nun, einen einzelnen Punkt $\mathbf{w}$ zu bestimmen, der die Verteilung für $y$
 am besten approximiert. Dieser Punkt wird \textit{Bayes Point} genannt. 
 Wie vorhin angedeutet ist dieser Punkt jedoch schwer zu finden. Wir wollen der Methode von Rujan (1997)
 folgen und die Poserior Erwartung $E[\mathbf{w}|D]$ für $\mathbf{w}$ benutzen. Im Folgenden nennen wir 
 dies den \textit{Bayes Point}. Unsere Strategie ist es eine Gaußapproximation für den Posterior zu machen und dann
 dessen Erwartung als \textit{Bayes Point} zu benutzen. 
 
 \subsection{Training via ADF}
 
 Auch hier starten wir mit dem ADF Updates. Um den ADF mit der \textit{Baye-Point-Machine} zu verknüpfen
 teilen wir die gemeinsame Verteilung von $P(D,\mathbf{w})$ in $n$ Terme. Dies entspricht einem Term für jeden
 Datenpunkt. Da $y$ und $\mathbf{x}$ stets verknüpft auftreten gehen wir in den folgenden Formeln davon aus, dass 
 $\mathbf{x_i}$ schon durch $y_i$ skaliert wurde. Wir nehmen an, dass der approximierte Posterior von der Form
 
 \begin{equation}
  q(\mathbf{w}) \sim \mathcal{N}(\mathbf{m}_w, \mathbf{V}_w)
 \end{equation}

 ist. Durch minimieren der KL-Divergenz erhalten wir die Beziehungen 
 
 \begin{equation}
 \begin{split}
  E_{q^{new}}[\mathbf{w}] &= E_{P'}[\mathbf{w}] \\
  E_{q^{new}}[\mathbf{w} \mathbf{w}^T] &= E_{P'}[\mathbf{w} \mathbf{w}^T] \\
 \end{split}
 \end{equation}

 Um diese Erwartungen berechnen zu können nutzen wir die folgenden Beziehungen, die wir du partielle Integration
 erhalten: 
 
 \begin{equation}
  \begin{split}
  Z(\mathbf{m}_w, \mathbf{V}_w) &= \int_w t(\mathbf{w}) q(\mathbf{w}) d \mathbf{w} \\
  E_{p'}[\mathbf{w}] &= \mathbf{m}_w \mathbf{V}_w \nabla_m \log Z(\mathbf{m_w}, \mathbf{V}_w) \\
  E_{P'}[\mathbf{w} \mathbf{w}^T] &= \mathbf{V}_w - \mathbf{V}_w (\nabla_m \nabla_m^T - 2 \nabla_v \log Z(\mathbf{m}_w, \mathbf{V}_w)) \mathbf{V}_w
  \end{split}
 \end{equation}

 Diese Beziehungen gelten für jedes $t(\mathbf{w})$. Für die \textit{Bayes-Point-Machine} erhalten wir indem wir 
 $y \mathbf{x}$ zu $\mathbf{x}$ kombinieren:
 
 \begin{equation}
  \begin{split}
  t(\mathbf{w}) &= \epsilon + (1 - 2 \epsilon) \Theta (\mathbf{w}^T \mathbf{x}) \\
  \phi(z) &= \int_{- \infty}^z \mathcal{N}(z;0,1) dz \\
  z &= \frac{\mathbf{m}_w^T \mathbf{x}}{\sqrt{\mathbf{x}^T \mathbf{V}_w \mathbf{x}}} \\
  Z(\mathbf{m}_w, \mathbf{V}_w) &= \epsilon + (1 - 2 \epsilon) \phi(z) \\
  \alpha &= \frac{1}{ \sqrt{ \mathbf{x}^T \mathbf{V}_w \mathbf{x} } } \frac{ (1 - 2 \epsilon) \mathcal{N}(z;0,1) }{ \epsilon + (1-2 \epsilon) \phi(z) } \\
  \nabla_m log Z(\mathbf{m}_w \mathbf{V}_w) &= \alpha \mathbf{x} \\
  \nabla_v log Z(\mathbf{m}_w \mathbf{V}_w) &= -\frac{1}{2} \frac{ \alpha \mathbf{m}_w^T \mathbf{x} }{\mathbf{x}^T \mathbf{V}_w \mathbf{x}} \mathbf{xx^T} \\
  \nabla_m \nabla_m^T - 2 \nabla_v \log Z(\mathbf{m}_w, \mathbf{V}_w) &= \alpha^2 \mathbf{x} \mathbf{x}^T + \frac{\alpha \mathbf{m}_w^T \mathbf{x}}{ \mathbf{x}^T \mathbf{V}_w \mathbf{x}} \mathbf{x x^T}
  \end{split}
 \end{equation}
  
  Damit ergibt sich für den ADF Algorithmus: 
  
  \begin{enumerate}
   \item Initialisiere $\mathbf{m}_w = 0, \mathbf{V}_w = \mathbf{1}$ (Prior). Initialisiere $s = 1$ (Skalierungsfaktor).
   \item Für jeden Datenpunkt $\mathbf{x}_i$ aktualisiere $(\mathbf{m}_w, \mathbf{V}_w), s$ mittels:
   \begin{equation}
    \begin{split}
      \mathbf{m}_w^{new} &= \mathbf{m}_w + \mathbf{V}_w \alpha_i \mathbf{x}_i \\
      \mathbf{V}_w^{new} &= \mathbf{V}_w - (\mathbf{V}_w \mathbf{x}_i) (\frac{ \alpha_i \mathbf{x}_i \mathbf{m}_w^{new}}{ \mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i }) (\mathbf{V}_w \mathbf{x}_i)^T \\
      \mathbf{s}^{new} &= s \times Z_i(\mathbf{m}_w, \mathbf{V}_w) \\
    \end{split}
   \end{equation}
  \end{enumerate}
   Nach Ausführung des Algorithmus ist $\mathbf{m}_w$ die Schätzung von $\mathbf{w}$, die zur Klassifizierung genutzt wird. Wir 
   möchten bemerken, dass dieser Algorithmus keine Matrixinvertierung benötigt, was erhebliche Rechenvorteile zur Foge hat.

  \subsection{Training via EP}
  
  In diesem Abschnitt modifizieren wir den ADF Algorithmus aus dem vorherigen Kapitel zu einem EP Algorithmus. 
  Durch das dividieren zwei konsekutiv gaußverteilter Posteriori, erhalten wir eine durch $\mathbf{m}_i$ und $\mathbf{V}_i$ parametrisierte,
  gaußverteilte Termapproximation: 
  
  \begin{equation}
  \begin{split}
   t'(\mathbf{w}) &= Z \frac{q^{new}(\mathbf{w})}{q(\mathbf{w})} \\ 
   &= \frac{Z}{\mathcal{N}(\mathbf{m}_i;\mathbf{m}_w;\mathbf{V}_i + \mathbf{V}_w)} \mathcal{N}(\mathbf{w}; \mathbf{m}_i,\mathbf{V}_i) \\
   \text{wobei } \mathbf{V}_i^{-1} &= (\mathbf{V}_w^{new})^{-1} - \mathbf{V}_w^{-1} \\ 
   \mathbf{m}_i &= \mathbf{V}_i (\mathbf{m}_w^{new})^{-1} \mathbf{m}_w^{new} - \mathbf{V}_i \mathbf{V}_w^{-1} \mathbf{m}_w \\
   &= \mathbf{m}_w + (\mathbf{V}_i \mathbf{V}_w) \mathbf{V}_w^{-1}(\mathbf{m}_w^{new} - \mathbf{m}_w) \\
  \end{split}
  \end{equation}
  
  Um den Ausdruck zu vereinfachen, kombinieren wir ihn mit den Gleichungen aus dem vorherigen Kapitel. Damit
  erhalten wir $(\mathbf{m}_i, \mathbf{V}_i)$ direkt aus $(\mathbf{m}_w, \mathbf{V}_w)$ : 
  
  \begin{equation}
  \begin{split}
   \mathbf{V}_i &= (\frac{ \alpha_i \mathbf{x}_i^T \mathbf{m}_w^{new} }{ \mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i } \mathbf{x}_i^T)^{-1} - \mathbf{V}_w \\
   \mathbf{m}_i &= \mathbf{m}_w + (\mathbf{V}_i + \mathbf{V}_w) \alpha_i \mathbf{x}_i 
   \end{split}  
  \end{equation}

  Die Eigenwerte der Matrix $(\frac{ \alpha_i \mathbf{x}_i^T \mathbf{m}_w^{new} }{ \mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i } \mathbf{x}_i^T)$ sind alle ungleich Null. 
  D.h., dass $\mathbf{V}_i$ einen endlichen Eigenwert in Richtung von $\mathbf{x}_i$ hat. 
  Dies macht Sinn, denn Term $i$ beschränkt nur die Projektion von $\mathbf{w}$ einlang von $\mathbf{x}_i$. 
  Diese spezielle Struktur erlaubt es uns $\mathbf{V}_i$ mithilfe eines Skalares $v_i$ darzustellen: 
  
  \begin{equation}
   \begin{split}
   \mathbf{V}_i^{-1} &= v_i \mathbf{x}_i \mathbf{x}_i^T \\
   \mathbf{x}_i^T \mathbf{V}_i \mathbf{x}_i &= v_i \\
  \end{split}
  \end{equation}
  
  Aus der Darstellung von $\mathbf{V}_i$, wissen wir 
  
  \begin{equation}
   \mathbf{x}_i^T \mathbf{V}_i \mathbf{x}_i =  \frac{\mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i}{\alpha_i \mathbf{x}_i^T \mathbf{m}_w^{new}} - \mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i
  \end{equation}

  was bedeutet, dass sich die Aktualiserung von $v_i$ als 
  
  \begin{equation}
   v_i = \mathbf{x}_i \mathbf{V}_w \mathbf{x}_i ( \frac{ 1 }{ \alpha_i \mathbf{x}_i^T \mathbf{m}_w^{new} } - 1 ) 
  \end{equation}
  
  ergibt. Eine weitere Konsequenz aus der speziellen Struktur ist, dass wir statt des vollständigen Vektors
  $\mathbf{m}_i$, nur die Projektion $\mathbf{m}_i^T \mathbf{x}_i$ benötigen. Auch diese kann in einem Skalar $m_i$ 
  gespeichert werden.
  
  Wir sind nun in der Lage einen effizienten EP Algorithmus zu formulieren: 
  
  \begin{enumerate}
   \item Außer für den Prior setzte die Termapproximationen auf
   \begin{equation}
    t_i'(\mathbf{w}) = s_i exp(- \frac{1}{2 v_i}(\mathbf{w}^T \mathbf{x}_i - m_i)^2)
   \end{equation}
   Initialisiere $v_i = \infty m _i = 0 s_i = 1$.
   \item Setzte $\mathbf{m}_w^{new} = 0, \mathbf{V}_w^{new} = \mathbf{I}$ (Prior).
   \item Bis alle $(m_i,v_i,s_i)$ konvergieren: \\
   Für $i = 1,...,n:$
   \begin{enumerate}
    \item Entferne $t_i'$ aus dem Posterior um einen früheren Posterior zu erhalten: 
    \begin{equation}
    \begin{split}
     \mathbf{V}_w &= ( (\mathbf{V}_w^{new})^{-1} - v_i^{-1} \mathbf{x}_i \mathbf{x}_i^T )^{-1} \\
     &= \mathbf{V}_w^{new} + (\mathbf{V}_w^{new} \mathbf{x}_i) ( v_i - \mathbf{x}_i^T \mathbf{V}_w^{new}\mathbf{x}_i )^{-1} (\mathbf{V}_w^{new} \mathbf{x}_i)^T \\
     \mathbf{m}_w &= \mathbf{m}_w^{new} + \mathbf{V}_w \mathbf{V}_i^{-1} (\mathbf{m}_w^{new} \mathbf{m}_i) \\
     &= \mathbf{m}_w^{new} + (\mathbf{V}_w \mathbf{x}_i) v_i^{-1} (\mathbf{x}_i^T \mathbf{m}_w^{new} - m_i) \\
     \end{split}
    \end{equation}
    Diese Ausdrücke können mithilfe von $\mathbf{V}_w$ effizient durch
    
    \begin{equation}
    \begin{split}
     \mathbf{V}_w \mathbf{x}_i &= ( \mathbf{V}_w^{new} \mathbf{x}_i ) \frac{ v_i }{ v_i - \mathbf{x}_i \mathbf{V}_w^{new} \mathbf{x}_i } \\
     \mathbf{x}_i \mathbf{V}_w \mathbf{x}_i &= (\frac{1}{\mathbf{x}_i \mathbf{V}_w^{new} \mathbf{x}_i} - \frac{1}{v_i})^{-1} \\
     \end{split}
    \end{equation}
    \item Berechne mittels ADF $(\mathbf{m}_w^{new}, \mathbf{V}_w^{new}, Z_i)$ aus $(\mathbf{m}_w, \mathbf{V}_w)$ neu.
    \item Aktualisiere $t_i'$: 
    \begin{equation}
     \begin{split}
      v_i &= \mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i ( \frac{1}{\alpha_i \mathbf{x}_i^T \mathbf{m}_w^{new}} -1 )  \\
      m_i &= \mathbf{x}_i^T \mathbf{m}_w + (v_i + \mathbf{x}_i^{T} \mathbf{V}_w \mathbf{x}_i) \alpha_i \\
      s_i &= Z_i \frac{|\mathbf{V}_i + \mathbf{V}_w|^{1/2}}{|\mathbf{V}_i|^{1/2}} exp(\frac{1}{2}(\mathbf{m}_i \mathbf{m}_w)^T) (\mathbf{V}_i + \mathbf{V}_w)^{-1} (\mathbf{m}_i - \mathbf{m}_w^))\\
      &= Z_i \sqrt{1 + v_i^{-1} \mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i} exp(\frac{1}{2} \frac{\mathbf{x}_i^T \mathbf{V}_w \mathbf{x}_i}{\mathbf{x}_i^T \mathbf{m}_w^{new}} \alpha_i)\\
      \end{split}
    \end{equation}
   \end{enumerate}
   \item Berechne die Normalisierungskonstante: 
   \begin{equation}
    \begin{split}
      B &= (\mathbf{m}_w^{new})^T (\mathbf{V}_w^{new})^{-1} (\mathbf{m}_w^{new}) - \sum_i \frac{m_i^2}{v_i} \\
      P(D) &\approx |\mathbf{V_w^{new}}|^{1/2} exp(B/2) \prod_{i = 1}^ s_i \\
     \end{split}
   \end{equation}
  \end{enumerate}

  Dieser Algorithmus verarbeitet jeden Datenpunkt in $\mathcal{O}(d^2)$. Unter der Annahme, dass die Anzahl 
  an Iterationen konstant ist benötigt das finden des \textit{Bayes-Point} mit diesem Algorithmus
   $\mathcal{O}(nd^2)$. Das Berechnen der Normalisierungskonstante am Ende braucht  $\mathcal{O}(d^3)$ Zeit. 

\section{Wahrscheinlichkeitstheorie}
Dieses Kapitel gibt einen Überblick über einige wichtige Konzepte der Wahrscheinlichkeitstheorie.
Im Folgenden werden Mengen mit Großbuchstaben, z.B. $X$, und deren Elemente mit Kleinbuchstaben, z.B $x$,
bezeichnet. Für Mengen definiere die Indikatorfunktion $I_{X}$ durch 
$$
I_{X}(x) := \begin{cases}0&\text{falls }x\notin X\\1&\text{falls }x \in X\end{cases}
$$

\begin{Definition}\textbf{($\sigma$-Algebra)}
 Sei eine Menge $\chi$ gegeben. Ein Mengensystem $\varUpsilon$ von Mengen $X \subseteq \chi$ wird 
 genau dann $\sigma$-Algebra über $\chi$ genannt, wenn 
 \begin{enumerate}
  \item Ist eine Menge X $\in$ $\varUpsilon$ enthalten, so auch ihr Komplement $X^{c} = \chi \backslash X$.
  \item Falls $X_{i} \in \varUpsilon,$ $i=1,...,\infty$ abzählbares Mengensystem in $\varUpsilon$
  ,dann sind auch $\cup_{i=1}^{\infty}X_{i} \in \varUpsilon$ und $\cap_{i=1}^{\infty}X_{i} \in \varUpsilon$ in 
  $\varUpsilon$ enthalten.
 \end{enumerate}

\end{Definition}

Kurz, jede $\sigma$-Algebra ist abgeschlossen unter Komplementbildung und abzählbaren Vereinigungen oder
Schnitten. 

\begin{Definition}\textbf{(Borelmengen)}
 Sei $\chi = \R^{n}$, die Borelmengen $B_{n}$ sind die kleinsten $\sigma$-Algebren, die alle 
 offenen Intervalle 
 $$
 \textit{\{}(x_{1},...,x_{n})\in \R^{n} | \forall i \in \textit{\{}1,...,n\textit{\}}: x_{i} \in (a_{i},b_{i})\textit{\}}
 $$
 für alle $a_{i},b_{i} \in \R$. Bemerke, dass $B_{n}$ überabzählbar ist. 
\end{Definition}

\begin{Definition}\textbf{(Maß- und Wahrscheinlichkeitsraum)}
 Ein messbarer Raum ist ein Tupel $(\chi,\varUpsilon)$. Dabei ist $\chi$ das Universum und 
 $\varUpsilon$ $\sigma$-Algebra über $\chi$. Ein Wahrscheinlichkeitsraum ist ein Tripel $(\chi,\varUpsilon,P)$,
 wobei $P$ ein Wahrscheinlichkeitsmaß auf $\chi$ ist. D.h. $P: \varUpsilon \rightarrow [0,1]$, sodass
 $P(\chi) = 1$ und für disjunkte abzählbare Vereinigungen $X_{i} \in \varUpsilon$, $i= 1,...\infty$ gilt
 $$
 P(\cup_{i=1}^{\infty}X_{i}) = \sum_{i=1}^{\infty}P(X_{i}).
 $$
 \end{Definition}

\begin{Definition}\textbf{(Messbarkeit)}
 Sei $(\chi,\varUpsilon)$ messbarer Raum. Eine reellwertige Funktion $g: \chi \rightarrow \R$
 heisst $\varUpsilon$-messbar (oder messbar) genau dann, wenn 
 $$
 \forall z \in \R: \textit{\{} x \in X | g(x) \leq z \textit{\}} \in \varUpsilon.
 $$
 \end{Definition}

\begin{Definition}\textbf{(Zufallsvariable)}
Sei $(\chi,\varUpsilon)$ messbarer Raum. Eine Zufallsvariable ist eine $\chi$-messbare reellwertige
Funktion $f: \chi \rightarrow \R$.
\end{Definition}

Eine Zufallsvariable $Y = f(X)$ induziert also ein Maß $P_{Y}$ auf $\R$, für das die 
$\sigma$-Algebra $A$ die Intervalle der Form {$(-\infty,z)|z \in \R$} enthält. Das Maß 
$P_{Y}$ ist vom Maß $P_{X}$ und $f$ indzuiert. Das bedeutet

$$
\forall Y \in B_{1}: P_{Y}(Y) := P_{X}(\textit{\{} x \in X | f(x) \in Y \textit{\}}).
$$

\begin{Definition}\textbf{(Verteilungsfunktion und Dichte)}
Für eine Zufallsvariable X heißt die durch 
$$
F_{X}(x) := P_{X}(X \leq x)
$$
definierte Funktion $F_{X}: \R \rightarrow [0,1]$ Verteilungsfunktion von $X$.
Die Funktion $f_{X}: \R \rightarrow \R$ wird Dichte genannt, falls 
$$
\forall z \in \R: F_{X}(z) = \int_{x \leq z} f_{X}(x) dx. 
$$
\end{Definition}

Für weitere Betrachtungen ist Erwartung einer Zufallsvariablen von essentieller Bedeutung. 

\begin{Definition}\textbf{(Erwartungswert)}
 Sei $f: \chi \rightarrow \R $ messbare Funktion. Der Erwartungswert $E_{X}[f(X)]$ von $f$
 über die Wahrscheinlichkeit von x wird durch 
 $$
 E_{X}[f(X)] := \int_{\R}f(x)dF_{X}(x)
 $$
 definiert. Der Erwartungswert ist nur dann definiert, wenn $\int_{\R}|f(x)|dF_{X}(x) < \infty$.
\end{Definition}

\begin{Definition}\textbf{(Varianz)}
 Die Varianz $Var(X)$ einer Zufallsvariable $X$ ist definiert durch 
 $$
 Var(X) := E_{X}[(X-\mu)^{2}] = E_{X}[X^{2}] - \mu^{2},
 $$
 wobei $\mu = E_{X}[X]$ der Erwartungswert der Zufallsvariable $X$ ist.  
\end{Definition}

\begin{Definition}\textbf{(Produktraum)}
 Seien zwei Maßräume $(\chi,\varUpsilon)$ und $(\varphi,\Phi)$ gegeben. Definiere den 
 Produktraum durch $(\chi \times \varphi,\varUpsilon \times \Phi)$. Hierbei bezeichnet
 $\varUpsilon \times \Phi$ die kleinste $\sigma$-Algebra welche die Mengen $\textit{\{}X \times Y | X \in \varUpsilon, Y \in \Phi\textit{\}}$.
 enthält.
 \end{Definition}

\begin{Definition}\textbf{(Marginal und bedingte Wahrscheinlichkeiten)}
  Sei $(\chi \times \varphi,\varUpsilon \times \Phi, P_{XY})$ der Produktraum von $X$ und $Y$.
  Das Marginalwahrscheinlichkeitsmaß $P_{X}$ ist dann durch 
  $$
  \forall X \in \varUpsilon: P_{X}(X) := P_{XY}(X \times \varphi)
  $$
  definiert.
  Sei $Y \in \Phi$ und $P_{Y}(Y) > 0$, dann ist das bedingte Wahrscheinlichkeitsmaß $P_{X|Y \in Y}$ durch
  $$
  \forall Y \in \varUpsilon: P_{X|Y}(X) := \frac{P_{XY}(X \times Y)}{P_{Y}(Y)}
  $$
  gegeben.
\end{Definition}

\begin{Definition}\textbf{Unabhängigkeit}
  Zwei Zufallsvariablen $X$ und $Y$ werden genau dann unabhängig genannt, wenn 
  $$
  \forall X \in \varUpsilon: \forall Y \in \Phi: P_{XY}(X \times Y) = P_{X}(X)P_{Y}(Y).
  $$
  In diesem Fall genügen die Marginalverteilungen um den gesamten Produktraum zu definieren.
\end{Definition}
  
  Im Folgenden schreibe \textbf{X}, falls \textbf{X} eine Folge $(X_{1},...,X{n})$ von Zufallsvariablen
  definiert. Eine solche Folge kann je nach Kontext entweder als Zeilen- oder als Spaltenvektor interpretiert 
  werden. Ein Element des Universums $\chi^{n}$ wird dann durch ein n-Tupel \textbf{x} beschrieben.
  Sei $\textbf{x} = (x_{1},...,x_{n})$ ein solches n-Tupel, dann verstehe $x \in \textbf{x}$
  als $\exists i \in \textit{\{} 1,...,n \textit{\}}: x_{i} = x$. 

\begin{Definition}\textbf{(Erwartungswert einer n-dimensionalen Zufallsvariable)}
 Seien $\textbf{X} = (X_{1},...,X_{n})$ n Zufallsvariablen mit gemeinsamem Wahrscheinlichkeitsmaß $P_{X}$, 
 dann ist die Erwartung $E_{\textbf{X}}[\textbf{X}]$ durch das n-Tupel
 $$
 E_{\textbf{X}}[\textbf{X}] = (E_{X_{1}}[X_{1}],...,E_{X_{n}}[X_{n}])
 $$
 gegeben.
\end{Definition}

\begin{Definition}\textbf{(Kovarianz und Konvarianzmatrix)}
 Seien $X$ und $Y$ zwei Zufallsvariablen mit gemeinsamem Wahrscheinlichkeitsmaß $P_{XY}$,
 dann ist die Kovarianz $Cov(X,Y)$ durch 
 $$
 Cov(X,Y) := E_{XY}[(X-\mu)(Y-\nu)]
 $$
 definiert, wobei $\mu = E_{X}[X]$ und $\nu = E_{Y}[Y]$. Es ist leicht einzusehemn,dass $Cov(X,X) = Var(X)$
 Sei $\textbf{X} = (X_{1},...,X_{n})$ eine Folge von n Zufallsvariablen und $\textbf{Y} = (Y_{1},...,Y_{n})$
 eine weitere solche Folge mit gemeinsamer Dichte $P_{XY}$, dann ist die $n \times m$ Kovarianzmatrix
 \textbf{Cov(X,Y)} definiert durch
 $$
 \textbf{Cov(X,Y)} := 
 \begin{pmatrix}
  Cov(X_{1},Y_{1}) 	& ... 	& Cov(X_{1},Y_{m})  	\\
  . 			& ... 	& .  			\\
  . 			& ... 	& .			\\
  . 			& ... 	& .		  	\\
  Cov(X_{n},Y_{1}) 	& ...	& Cov(X_{n},Y_{m}) 
 \end{pmatrix}
 $$
 Falls $\textbf{X} = \textbf{Y}$ $\textbf{Cov(X,X)} = \textbf{Cov(X)}$
 \end{Definition}
 
 \subsection{Eigenschaften von Zufallsvariablen}
 
 \begin{Satz}\textbf{(Erwartungswert von Summen und Produkten)}
  Seien $X$ und $Y$ zwei unabhängige Zufallsvariablen, so gilt
  \begin{equation}
  E_{XY}[X \cdot Y] = E_{X}[X] \cdot E_{Y}[Y], 
  \end{equation}
\begin{equation}
  E_{XY}[X + Y] = E_{X}[X] + E_{Y}[Y], 
  \end{equation}
  immer dann, wenn die Terme auf der rechten Seite existieren. Die zweite Aussage gilt sogar, falls
  $X$ und $Y$ nicht unabhängig sind. 
 \end{Satz}

 \begin{Satz}\textbf{(Linearität des Erwartungswertes)}
  Für jede n-dimensionale Zufallsvariable $\textbf{X}$, jede Matrix $\textbf{A} \in \R^{m \times n}$
  und jeden stationären Vektor $\textbf{b} \in \R^{m}$ gilt
  $$
  E_{X}[\textbf{AX} + \textbf{b}] = \textbf{A}E_{X}[\textbf{X} + \textbf{b}].
  $$
  \end{Satz}
  
  \begin{Satz}\textbf{Varianz Zerlegung}
   Seien $X$ und $Y$ zwei unabhängige Zufallsvariablen, dann gilt 
   $$
   Var(X + Y) = Var(X) + Var(Y)
   $$
  \end{Satz}
  \begin{Beweis}
   Setze $\mu = E_{X}[X]$ und $\nu = E_{Y}[Y]$. Benutze die Definition der Varianz
   $$
   \begin{array}{c}
     E_{XY}[(X+Y-E_{XY}[X+Y])^{2}] = E_{XY}[((X-\mu)(Y-\nu))^{2}] \\
     = E_{XY}[(X-\mu)^{2}+ 2 \cdot (X-\mu)(Y-\nu) + (Y-\nu)^{2}]  \\
     = E_{X}[(X-\mu)^{2}] + 2E_{XY}[(X-\mu)(Y-\nu)] + E_{Y}[(Y-\nu)^{2}] \\
     = E_{X}[(X-\mu)^{2}] + 2E_{X}[(X-\mu)]E_{Y}[(Y-\nu)] + E_{Y}[(Y-\nu)^{2}] \\
     = Var(X) + Var(Y)
   \end{array}
   $$
  \end{Beweis}

  \begin{Lemma}
   Für jede Zufallsvariable $X$ und jede Konstant $c \in \R$ gilt \\ 
   $Var(cX) = c^{2} \cdot Var(X)$.
  \end{Lemma}

\subsection{Mehrdimensionale Gauß Verteilung}

In diesem Abschnitt werden einige der wichtigsten Eigenschaften der Gaussverteilung zusammengefasst.

\begin{Definition} Sei $\mu \in \R^n$ ein Vektor und $A \in \R^{nxm}$ eine deterministische Matrix. 
Sei weiter $ Y = (Y_{1},...,Y_{m})$ eine Folge von m unabhängigen, normalverteilten Zufallsvariablen
$Y_{i}$ mit Mittewert Null und Einheitsvarianz ($Y_{i} \sim Normal(0,1)$). Dann wird $X = AY + \mu$ normal-
oder gaußverteilt mit Erwartungswert $E_{X}[X] = \mu$ und Kovarianzmatrix $Cov(X) = \varSigma = AA^{'}$ genannt.
Da das Maß $P_{X}$ eindeutig durch diese beiden Größen beschrieben wird schreiben wir im Weiteren auch $Y \sim Normal(\mu , \varSigma)$.
\end{Definition}

\begin{Satz} Falls $X \sim Normal(\mu,\varSigma)$, dann besitzt $X$ genau dann eine Dichte $f_X$, wenn 
$\varSigma$ positiv definit ist. Die Dichte $f_{X}$ ist gegeben durch 
\begin{equation}
f_{X}(x) = \frac{1}{(2\pi)^{n/2}|\varSigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu)^{T}\varSigma^{-1}(x-\mu)) 
\end{equation}

\end{Satz}

\begin{Satz}
 Sei $X \sim Normal(\mu,\varSigma)$ eine n-dimensional normalverteilte Zufallsvariable,
 $A \in R^{mxn}$ stationäre Matrix und $b \in \R^m$ stationärer Vektor. Dann ist auch die Zufallsvariable 
 $Y = AX + b$ normalverteilt mit $Y \sim Normal(A\mu + b, A\varSigma A^{T})$.
\end{Satz}

\begin{Satz}
 Angenommen $P_{X|Y=y}=Normal(Xy,\Gamma)$ ist normalverteiltes Wahrscheinlichkeitsmaß, wobei 
 $X \in \R^{mxn}$ und $\Gamma \in \R^{mxm}$ stationäre Matrizen für alle Werte $y \in \R^{n}$ sind. Falls
 $P_{Y} = Normal(\mu,\varSigma)$ normalverteiltes Wahrscheinlichkeitsmaß ist, so gilt
 
 \begin{equation}
 P_{Y|X=x} = Normal(\Psi(X^{T}\Gamma^{-1}x+\varSigma^{-1}\mu),\Psi).
 \end{equation}
 \begin{equation}
  P_{X} = Normal(X\mu,\Gamma+X\varSigma X^{T}).
 \end{equation}
 ,wobei $\Psi = (X^{T}\Gamma^{-1}X+\varSigma^{-1})^{-1}$
\end{Satz}

\begin{Beweis}
 Nach Satz () wissen wir, dass 
$$
  f_{Y|X=x(y)} = \frac{f_{X|Y=y}f_{Y}(y)}{\int_{\R^n}f_{X|Y=y^{'}(x)}f_{Y}(y^{'})dy^{'}} 
 = \frac{f_{X|Y=y}(x)f_{Y}(y)}{f_{X}(x)}.
$$

 Es fällt auf, dass der Nenner unabhängig von $y$ ist. Betrachte nun deshalb zunächst den Zähler.
 Mithilfe von Definition ist Letzteres gegeben durch 
 $$ 
 c\cdot exp(-\frac{1}{2}((x-Xy)^{T}\Gamma^{-1}(x-Xy)+(y-\mu)^{T}\varSigma^{-1}(y-\mu))),
 $$

 
 wobei $c = (2\pi)^{\frac{-m+n}{2}}|\Gamma|^{-\frac{1}{2}}|\Gamma|^{-\frac{1}{2}}$ unabhängig von 
 $x$ und $y$ ist.
 Diesen Ausdruck wiederum können wir umschreiben als 
 
 $$
 c \cdot exp(-\frac{1}{2}((y-x)^{T}C(y-c)+d(x))), 
 $$
 mit 
 
 $$
 C = X^{T}\Gamma^{-1}X+\varSigma^{-1},
 $$
 
 $$
 Cc = X^{T}\Gamma^{-1}X+\varSigma^{-1}\mu,
 $$
 
 $$
 d(x) = (x-X\mu)^{T}(\Gamma+X\varSigma X^{T})^{-1}(x-X\mu).
 $$
 
 Da $d(x)$ als Funktion nicht von $y$ abhängt, kann der Term $exp(-\frac{1}{2}d(x))$ mit in die
 Konstante c gezogen werden und somit folgt die erste Gleichung des Satzes indem $\Psi:=C^{-1}$
 gesetzt wird.\\
 Um die Zweite Aussage zu zeigen kann die Definition von $f_{X}(x)$ benutzt werden, d.h,
 
 $$
 \begin{array}{c}
  f_{X}(x) = \int_{\R^{n}}c \cdot exp(-\frac{1}{2}((y^{'}-c)^{T}C(y^{'}-c)+d(x))dy^{'} \\
 = c \cdot exp(-\frac{1}{2}d(x)) \cdot \int_{\R^{n}}exp(-\frac{1}{2}((y^{'}-c)^{T}C(y^{'}-c))dy^{'}\\
 = c \cdot exp(-\frac{1}{2}d(x)) \cdot (2\pi)^{\frac{n}{2}}|C|^\frac{1}{2} = c^{'} \cdot exp(-\frac{1}{2}d(x))\\
 = c^{'} \cdot exp(-\frac{1}{2}(x-X\mu)^{T}(\Gamma+X\varSigma X^T)^{-1}(x-X\mu)),
 \end{array}
 $$
 wobei die dritte Zeile aus der Definition und der Tatsache folgt, dass sich Wahrscheinlichkeitsdichten
 zu Einer zusammenfassen lassen. Dies zeigt die zweite Aussage. 
\end{Beweis}


   




  % Literaturverzeichnis (beginnt auf einer ungeraden Seite)
  \newpage
\begin{thebibliography}{Lam00}
 
\end{thebibliography}
 
      
  % ggf. hier Tabelle mit Symbolen 
  % (kann auch auf das Inhaltsverzeichnis folgen)

\newpage
  
 \thispagestyle{empty}


\vspace*{8cm}


\section*{Erklärung}

Hiermit versichere ich, dass ich diese Arbeit selbständig verfasst und keine anderen, als die angegebenen Quellen und Hilfsmittel benutzt, die wörtlich oder inhaltlich übernommenen Stellen als solche kenntlich gemacht und die Satzung des Karlsruher Instituts für Technologie zur Sicherung guter wissenschaftlicher Praxis in der jeweils gültigen Fassung beachtet habe. \\[2ex] 

\noindent
Ort, den Datum\\[5ex]

% Unterschrift (handgeschrieben)



\end{document}

